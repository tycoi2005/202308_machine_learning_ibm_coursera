{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization\n",
    "\n",
    "This module introduces matrix factorization, which is a powerful technique for big data, text mining, and pre-processing data.\n",
    "\n",
    "Learning Objectives\n",
    "- Become familiar with the scikit learn syntax for Non Negative Matrix Factorization\n",
    "- Explain non negative matrix factorization, and how it makes PCA difficult with many features\n",
    "\n",
    "## Non Negative Matrix Factorization\n",
    "\n",
    "![](./images/27_NonNegativeMatrixFactorization.png)\n",
    "\n",
    "### Why Only Positive Values?\n",
    "Since NMF can never undo the application of a latent feature,\n",
    "it is much more careful about what it adds at each step.\n",
    "\n",
    "In some applications, this can make for more human interpretable latent features.\n",
    "\n",
    "Because NMF has the extra constraint of positive values,\n",
    "it will tend to lose more information when truncating.\n",
    "\n",
    "Also, NMF does not have to give orthogonal latent vectors.\n",
    "\n",
    "\n",
    "### NMF Summary\n",
    "Input:\n",
    "- Count Vectorizer or TF-IDF Vectorizer\n",
    "\n",
    "Parameters to Tune:\n",
    "- Number of Topics\n",
    "- Text Preprocessing (stop words, min / max doc freq, parts of speech...)\n",
    "\n",
    "Output:\n",
    "- W Matrix (terms topics) and H Matrix (documents -> topics)\n",
    "\n",
    "### NMF: the Syntax\n",
    "Import the class containing the clustering method.\n",
    "```python\n",
    "from sklearn.decomposition import NMF \n",
    "```\n",
    "Create an instance of the class.\n",
    "```python\n",
    "nmf = NMF(n_components=3, init=\"random\")\n",
    "```\n",
    "Fit the instance and create transformed version of the data:\n",
    "\n",
    "```python\n",
    "X_nmf = nmf.fit(X)\n",
    "```\n",
    "\n",
    "## Dimensionality Reduction: Approaches\n",
    "\n",
    "Dimensionality reduction is common across a wide range of applications\n",
    "Some rules of thumb for selecting an approach:\n",
    "\n",
    "| Method                              | Use case                                                                                                                             |\n",
    "|-------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Principal Components Analysis (PCA) | Identify small number of transformed variables with different effects, preserving variance                                           |\n",
    "| Kernel PCA                          | Useful for situations with nonlinear relationships, but requires more computation than PCA                                           |\n",
    "| Multidimensional Scaling            | Like PCA, but new (transformed features) are determined based on preserving distance between points, rather than explaining variance |\n",
    "| Non-negative Matrix Factorization   | Useful when you want to consider only positive values (word matrices, images)                                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "## Non Negative Matrix Decomposition\n",
    "Non Negative Matrix Decomposition is another way of reducing the number of dimensions. Similar to PCA, it is also a matrix decomposition method in the form V=WxH.\n",
    "\n",
    "The main difference is that it can only be applied to matrices that have positive values as inputs, for example:\n",
    "\n",
    "- pixels in a matrix\n",
    "\n",
    "- positive attributes that can be zero or higher\n",
    "\n",
    "In the case of word and vocabulary recognition, each row in the matrix can be considered a document, while each column can be considered a topic.\n",
    "\n",
    "NMF has proven to be powerful for:\n",
    "\n",
    "- word and vocabulary recognition\n",
    "\n",
    "- image processing, \n",
    "\n",
    "- text mining\n",
    "\n",
    "- transcribing\n",
    "\n",
    "- encoding and decoding\n",
    "\n",
    "- decomposition of video, music, or images\n",
    "\n",
    "There are advantages and disadvantages of only dealing with non negative values.\n",
    "\n",
    "An advantage, is that NMF leads to features that tend to be more interpretable. For example, in facial recognition, the decomposed components match to something more interpretable like, for example, the nose, the eyebrows, or the mouth.\n",
    "\n",
    "A disadvantage is that NMF truncates negative values by default to impose the added constraint of only positive values. This truncation tends to lose more information than other decomposition methods.\n",
    "\n",
    "Unlike PCA, it does not have to use orthogonal latent vectors, and can end up using vectors that point in the same direction.\n",
    "\n",
    "## NMF for NLP\n",
    "In the case of Natural Language Processing, NMF works as below given these inputs, parameters to tune, and outputs:\n",
    "\n",
    "### Inputs\n",
    "\n",
    "Given vectorized inputs, which are usually pre-processed using count vectorizer or vectorizers in the form of Term Frequency - Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "### Parameters to tune\n",
    "\n",
    "The main two parameters are:\n",
    "\n",
    "- Number of Topics\n",
    "\n",
    "- Text Preprocessing (stop words, min/max document frequency, parts of speech, etc)\n",
    "\n",
    "### Output\n",
    "\n",
    "The output of NMF will be two matrices:\n",
    "\n",
    "W Matrix telling us how the terms relate to the different topics.\n",
    "\n",
    "H Matrix telling us how to use those topics to reconstruct our original documents.\n",
    "\n",
    "### Syntax\n",
    " The syntax consists of importing the class containing the clustering method:\n",
    "\n",
    "   from sklearn.decomposition import NMF\n",
    "\n",
    " creating the instance of the class:\n",
    "\n",
    "    nmf=NMF(n_components=3, init='random')\n",
    "\n",
    "and fit the instance and create a transformed version of the data:\n",
    "\n",
    "   x_nmf=NMF.fit(X)  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

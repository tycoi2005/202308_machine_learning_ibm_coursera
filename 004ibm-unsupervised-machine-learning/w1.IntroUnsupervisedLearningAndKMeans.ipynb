{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and K Means\n",
    "\n",
    "This module introduces Unsupervised Learning and its applications. One of the most common uses of Unsupervised Learning is clustering observations using k-means. In this module, you become familiar with the theory behind this algorithm, and put it in practice in a demonstration.\n",
    "\n",
    "## Learning Objectives\n",
    "- Explain the kinds of problems suitable for Unsupervised Learning approaches\n",
    "- Describe the clustering process of the k-means algorithm\n",
    "- Become familiar with k-means clustering syntax in scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Unsupervised Learning\n",
    "\n",
    "- Data points have unknown outcome\n",
    "\n",
    "Types:\n",
    "\n",
    "- Clustering: identify unknown structure in data\n",
    "  - K-Means\n",
    "  - Hierarchical Agglomerative Clustering\n",
    "  - Mean Shift\n",
    "  - DBSCAN\n",
    "\n",
    "- Dimensionality reduction: use structural characteristics to simplify data\n",
    "  -  Namely using structural characteristics to reduce the size of our dataset without losing much information contained in that original dataset.\n",
    "  - Principal Components Analysis\n",
    "  - Non-negative Matrix Factorization\n",
    "\n",
    "## Curse of Dimensionality\n",
    "- In theory, increasing features should improve performance.\n",
    "- In practice, too many features leads to worse performance.\n",
    "\n",
    "Number of training examples required increases exponentially with dimensionality.\n",
    "- 1 dimension: 10 positions \n",
    "- 2 dimensions: 100 positions \n",
    "- 3 dimensions: 1000 positions\n",
    "\n",
    "## Curse of Dimensionality: Churn Example\n",
    "\n",
    "The Curse of Dimensionality comes up often in applications.\n",
    "\n",
    "Consider the customer churn example from earlier.\n",
    "\n",
    "The original dataset has 54 columns:\n",
    "\n",
    "- Some, like 'Age', 'Under 30', and 'Senior Citizen' are\n",
    "closely related.\n",
    "- Others (Latitude for example) are essentially duplicated.\n",
    "- Even if we remove duplicates and non-numeric columns,\n",
    "the curse of dimensionality applies\n",
    "\n",
    "Clustering can help identify groups of similar customers\n",
    "\n",
    "Dimensionality reduction can improve both the performance and\n",
    "interpretability of this grouping\n",
    "\n",
    "![](./images/01_UnsupervisedLearningOverview.png)\n",
    "\n",
    "![](./images/02_ClusteringExample.png)\n",
    "\n",
    "## Common Clustering Use Cases\n",
    "\n",
    "Classification\n",
    "\n",
    "Anomaly detection\n",
    "\n",
    "Customer segmentation\n",
    "\n",
    "Improve supervised learning\n",
    "\n",
    "## Common Dimesion Reduction Use Cases\n",
    "\n",
    "Image processing: \n",
    "- High relusotion images -> compressed images\n",
    "\n",
    "Image tracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering\n",
    "\n",
    "## K Means Algorithm\n",
    "\n",
    "K=2\n",
    "\n",
    "- we're going to initialize our algorithm by picking 2 random points. And these are going to act as the centroids of our clusters\n",
    "- Then with our centroids initiated, we take each example in our space, and determine which cluster it belongs to by computing the distance to the nearest centroid, and seeing which one's closer.\n",
    "- So the second step is then to adjust the points, to adjust those centroids that we just discussed to the new mean of our clusters. \n",
    "- By continuously iterating, moving to the mean of those identified points that were closest, until it was not able to move anymore. Those centuries stayed in place, and we have our two clusters.\n",
    "\n",
    "K=3\n",
    "\n",
    "- there can be multiple solutions, depends on initial points\n",
    "\n",
    "### Smarter Initializion\n",
    "- Random 1 point\n",
    "- Pick next point with probability $ distance (x_i)^2/ \\sum_{i=1}^n distance(x_i)^2 $ (far point from first point)\n",
    "- ... pick next point far from others\n",
    "- \n",
    "\n",
    "## Selecting the Right Number of Clusters in K-Means\n",
    "\n",
    "Sometimes the question has a K:\n",
    "- Clustering similar jobs on 4 CPU cores (K=4)\n",
    "- A clothing design in 10 different sizes to cover most people (K=10)\n",
    "- A navigation interface for browsing scientific papers with 20 disciplines (K=20)\n",
    "\n",
    "Often, the number of clusters (K) is unclear, and we need an approach to select it.\n",
    "\n",
    "\n",
    "### Evaluating Clustering Performance\n",
    "\n",
    "Inertia: sum of squared distance from each point $(x_i)$ to its cluster $(C_k)$.\n",
    "$$\n",
    "\\sum_{i=1}^n(x_i-C_k)^2\n",
    "$$\n",
    "\n",
    "- Smaller value corresponds to tighter clusters.\n",
    "\n",
    "- Value sensitive to number of points in cluster.\n",
    "\n",
    "- And if you're more concerned that clusters have similar numbers of points, then you should use inertia.\n",
    "\n",
    "Distortion: average of squared distance from each point $(x_i)$ to its cluster $(C_k)$.\n",
    "$$\n",
    "\\dfrac{1}{n} \\sum_{i=1}^n(x_i-C_k)^2\n",
    "$$\n",
    "\n",
    "- Smaller value corresponds to tighter clusters.\n",
    "- Doesn't generally increase as more points are added (relative to Inertia)\n",
    "- When the similarity of points in the cluster is more important, you should use distortion\n",
    "\n",
    "So what can we do in order to find the clustering with best inertia? What we would do is, we initiate our K means algorithm several times. And with different initial configurations, and with that, assuming we predefined what our K is, we can compute the resulting inertia or distortion. Keep that results and see which one of our different initializations or configurations lead to the best inertia or distortion. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method and Applying K-means\n",
    "\n",
    "![](./images/03_ChoosingRightNumberOfCluster.png)\n",
    "\n",
    "### K-Means: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the clustering method.\n",
    "from sklearn. cluster import KMeans\n",
    "\n",
    "# Create an instance of the class.\n",
    "kmeans = KMeans(n_clusters = 3, init='k-means++')\n",
    "\n",
    "#Fit the instance on the data and then predict clusters for new data.\n",
    "kmeans = kmeans.fit(X1)\n",
    "y_predict = kmeans.predict(X2)\n",
    "\n",
    "# Can also be used in batch mode with MiniBatchKMeans.\n",
    "```\n",
    "\n",
    "### K-Means: Elbow Method Syntax\n",
    "\n",
    "To implement elbow method, fit K-Means for various levels of k, save inertia values.\n",
    "\n",
    "```python\n",
    "inertia = [ ]\n",
    "list_clusters = list(range(10))\n",
    "for k in list_clusters:\n",
    "  kmeans = KMeans(n_clusters=k)\n",
    "  kmeans.fit(X)\n",
    "  inertia.append(km.inertia_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "## Unsupervised Learning Algorithms\n",
    "Unsupervised algorithms are relevant when we don’t have an outcome or labeled variable we are trying to predict.\n",
    "\n",
    "They are helpful to find structures within our data set and when we want to partition our data set into smaller pieces.   \n",
    "\n",
    "Types of Unsupervised Learning:\n",
    "\n",
    "| Type of Unsupervised Learning | Data                                                    | Example                                                                      | Algorithms                                                         |\n",
    "|-------------------------------|---------------------------------------------------------|------------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
    "| Clustering                    | Use unlabeled data, Identify unknown structures in data | Segmenting customers into different groups                                   | K-means, Hierarchical Agglomerative Clustering, DBSCAN, Mean shift |\n",
    "| Dimensionality Reduction      | Use structural characteristics to simplify data         | Reducing size without losing too much information from our original data set | Principal Components Analysis, Non-negative Matrix, Factorization  |\n",
    "\n",
    "Dimensionality reduction is important in the context of large amounts of data.\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "In theory, a large number of features should improve performance.  As models have more data to learn from, they should be more successful. But in practice, too many features lead to worse performance. There are several reasons why too many features end up leading to worse performance. If you have too many features, several things can be wrong, for example: \n",
    "\n",
    "-        Some features can be spurious correlations, which means they correlate into the data set but not outside your data set, as long as new data comes in. \n",
    "\n",
    "-        Too many features create more noise than signal.\n",
    "\n",
    "-        Algorithms find it hard to sort through non-meaningful features if you have too many features. \n",
    "\n",
    "-        The number of training examples required increases exponentially with dimensionality.\n",
    "\n",
    "-        Higher dimensions slows performance.\n",
    "\n",
    "-        Larger data sets are computationally more expensive.\n",
    "\n",
    "-        Higher incidence of outliers. \n",
    "\n",
    "To fix these problems in real life, it's best to reduce the dimension of the data set. \n",
    "\n",
    "Similar to feature selection, you can use Unsupervised Machine Learning models such as Principal Components Analysis.\n",
    "\n",
    "## Common uses of clustering cases in the real world\n",
    "1.     Anomaly detection\n",
    "\n",
    "Example: Fraudulent transactions.\n",
    "\n",
    "Suspicious fraud patterns such as small clusters of credit card transactions with high volume of attempts, small amounts, for new merchants. This creates a new cluster and this is presented as an anomaly so perhaps there’s fraudulent transactions happening. \n",
    "\n",
    "2.     Customer segmentation\n",
    "\n",
    "You could segment the customers by recency, frequency, and average amount of visits in the last 3 months. Another common type of segmentation is by demographic and the level of engagement, for example, single costumers, new parents, empty nesters, etc. And the combinations of each with the preferred marketing channel, so you can use these insights for future marketing campaigns. \n",
    "\n",
    "3.      Improve supervised learning\n",
    "\n",
    "You can perform a Logistic regression for each cluster. This means training one model for each segment of your data to try to improve the classification.\n",
    "\n",
    "## Common uses of Dimension Reduction in the real world\n",
    "\n",
    "1. Turn high-resolution images into compressed images\n",
    "\n",
    "This means to come to a reduced, more compact version of those images, so they can still contain most of the data that can tell us what the image is about.  \n",
    "\n",
    "2.  Image tracking\n",
    "\n",
    "Reduce the noise to the primary factors that are relevant in a video capture. The benefits of reducing the data set can greatly speed up the computational efficiency of the detection algorithms.   \n",
    "\n",
    "## K-means Clustering\n",
    "K-means clustering is an iterative process in which similar observations are grouped together. To do that, this algorithm starts by taking 2 random points known as centroids, and starts calculating the distance of each observation to the centroid, and assigning each cluster to the nearest centroid. After the first iteration, every point belongs to a cluster.\n",
    "\n",
    "Next, the number of centroids increases by one, and the centroid for each cluster is recalculated as the points with the average distance to all points in a given cluster. Then, we keep repeating this process until no example is assigned to another cluster. \n",
    "\n",
    "And this process is repeated k-times, hence the name k-means. This algorithm converges when clusters do not move anymore.\n",
    "\n",
    "We can also create multiple clusters, and we can have multiple solutions. By multiple solutions, we mean that the clusters are not going to move anymore (they converged), but we can converge in different places, where we no longer move those centroids.\n",
    "\n",
    "## Advantages and Disadvantages of K-Means  \n",
    "\n",
    "The main advantage of k-means algorithm is that it is easy to compute. One disadvantage is that this algorithm is sensitive to the choice of the initial points, so different initial configurations may yield different results. \n",
    "\n",
    "To overcome this, there is a smarter initialization of K-mean clusters called K-means ++, which helps to avoid getting stuck at local optima. This is the default implementation of the K-means.     \n",
    "\n",
    "## Model Selection, choosing K number of clusters\n",
    "\n",
    "Sometimes you want to split your data into a predetermined number of groups or segments. Often, the number of clusters (K) is unclear, and you need an approach to select it.\n",
    "\n",
    "A common metric is Inertia, defined as the sum of squares distance from each point to its cluster centroid.\n",
    "\n",
    "Smaller values of Inertia correspond to tighter clusters, this means that we are penalizing spread out clusters and rewarding clusters that are tighter to their centroids.\n",
    "\n",
    "The drawback of this metric is that its value sensitive to number of points in clusters. The more points you add, the more you will continue penalizing the inertia of a cluster, even if those points that are relatively closer to the centroids than the existing points. \n",
    "\n",
    "Another metric is Distortion, defined as the average of squared distance from each point to its cluster.\n",
    "\n",
    "Smaller values of distortion corresponds to tighter clusters.\n",
    "\n",
    "An advantage of distortion is that it doesn’t generally increase, as more points are added (relative to inertia). This means that it doesn’t increase distortion, as closer points will actually decrease the average distance to the cluster centroid.\n",
    "\n",
    "## Inertia Vs. Distortion \n",
    "\n",
    "Both Inertia and Distortion are measures of entropy per cluster.\n",
    "\n",
    "Inertia will always increase, as more members are added to each cluster, while this will not be the case with distortion. \n",
    "\n",
    "When the similarity of the points in the cluster are very relevant, you should use distortion and if you are more concerned that clusters should have a similar number of points, then you should use inertia.     \n",
    "\n",
    "## Finding the right cluster\n",
    "To find the cluster with a low entropy metric, you can run a few k-means clustering models with different initial configurations, compare the results, and determine which one of the different initializations of configurations leads to the lowest inertia or distortion."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

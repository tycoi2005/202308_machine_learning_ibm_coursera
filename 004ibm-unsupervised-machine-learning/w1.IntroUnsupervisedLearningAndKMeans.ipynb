{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and K Means\n",
    "\n",
    "This module introduces Unsupervised Learning and its applications. One of the most common uses of Unsupervised Learning is clustering observations using k-means. In this module, you become familiar with the theory behind this algorithm, and put it in practice in a demonstration.\n",
    "\n",
    "## Learning Objectives\n",
    "- Explain the kinds of problems suitable for Unsupervised Learning approaches\n",
    "- Describe the clustering process of the k-means algorithm\n",
    "- Become familiar with k-means clustering syntax in scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Unsupervised Learning\n",
    "\n",
    "- Data points have unknown outcome\n",
    "\n",
    "Types:\n",
    "\n",
    "- Clustering: identify unknown structure in data\n",
    "  - K-Means\n",
    "  - Hierarchical Agglomerative Clustering\n",
    "  - Mean Shift\n",
    "  - DBSCAN\n",
    "\n",
    "- Dimensionality reduction: use structural characteristics to simplify data\n",
    "  -  Namely using structural characteristics to reduce the size of our dataset without losing much information contained in that original dataset.\n",
    "  - Principal Components Analysis\n",
    "  - Non-negative Matrix Factorization\n",
    "\n",
    "## Curse of Dimensionality\n",
    "- In theory, increasing features should improve performance.\n",
    "- In practice, too many features leads to worse performance.\n",
    "\n",
    "Number of training examples required increases exponentially with dimensionality.\n",
    "- 1 dimension: 10 positions \n",
    "- 2 dimensions: 100 positions \n",
    "- 3 dimensions: 1000 positions\n",
    "\n",
    "## Curse of Dimensionality: Churn Example\n",
    "\n",
    "The Curse of Dimensionality comes up often in applications.\n",
    "\n",
    "Consider the customer churn example from earlier.\n",
    "\n",
    "The original dataset has 54 columns:\n",
    "\n",
    "- Some, like 'Age', 'Under 30', and 'Senior Citizen' are\n",
    "closely related.\n",
    "- Others (Latitude for example) are essentially duplicated.\n",
    "- Even if we remove duplicates and non-numeric columns,\n",
    "the curse of dimensionality applies\n",
    "\n",
    "Clustering can help identify groups of similar customers\n",
    "\n",
    "Dimensionality reduction can improve both the performance and\n",
    "interpretability of this grouping\n",
    "\n",
    "![](./images/01_UnsupervisedLearningOverview.png)\n",
    "\n",
    "![](./images/02_ClusteringExample.png)\n",
    "\n",
    "## Common Clustering Use Cases\n",
    "\n",
    "Classification\n",
    "\n",
    "Anomaly detection\n",
    "\n",
    "Customer segmentation\n",
    "\n",
    "Improve supervised learning\n",
    "\n",
    "## Common Dimesion Reduction Use Cases\n",
    "\n",
    "Image processing: \n",
    "- High relusotion images -> compressed images\n",
    "\n",
    "Image tracking\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering\n",
    "\n",
    "## K Means Algorithm\n",
    "\n",
    "K=2\n",
    "\n",
    "- we're going to initialize our algorithm by picking 2 random points. And these are going to act as the centroids of our clusters\n",
    "- Then with our centroids initiated, we take each example in our space, and determine which cluster it belongs to by computing the distance to the nearest centroid, and seeing which one's closer.\n",
    "- So the second step is then to adjust the points, to adjust those centroids that we just discussed to the new mean of our clusters. \n",
    "- By continuously iterating, moving to the mean of those identified points that were closest, until it was not able to move anymore. Those centuries stayed in place, and we have our two clusters.\n",
    "\n",
    "K=3\n",
    "\n",
    "- there can be multiple solutions, depends on initial points\n",
    "\n",
    "### Smarter Initializion\n",
    "- Random 1 point\n",
    "- Pick next point with probability $ distance (x_i)^2/ \\sum_{i=1}^n distance(x_i)^2 $ (far point from first point)\n",
    "- ... pick next point far from others\n",
    "- \n",
    "\n",
    "## Selecting the Right Number of Clusters in K-Means\n",
    "\n",
    "Sometimes the question has a K:\n",
    "- Clustering similar jobs on 4 CPU cores (K=4)\n",
    "- A clothing design in 10 different sizes to cover most people (K=10)\n",
    "- A navigation interface for browsing scientific papers with 20 disciplines (K=20)\n",
    "\n",
    "Often, the number of clusters (K) is unclear, and we need an approach to select it.\n",
    "\n",
    "\n",
    "### Evaluating Clustering Performance\n",
    "\n",
    "Inertia: sum of squared distance from each point $(x_i)$ to its cluster $(C_k)$.\n",
    "$$\n",
    "\\sum_{i=1}^n(x_i-C_k)^2\n",
    "$$\n",
    "\n",
    "- Smaller value corresponds to tighter clusters.\n",
    "\n",
    "- Value sensitive to number of points in cluster.\n",
    "\n",
    "- And if you're more concerned that clusters have similar numbers of points, then you should use inertia.\n",
    "\n",
    "Distortion: average of squared distance from each point $(x_i)$ to its cluster $(C_k)$.\n",
    "$$\n",
    "\\dfrac{1}{n} \\sum_{i=1}^n(x_i-C_k)^2\n",
    "$$\n",
    "\n",
    "- Smaller value corresponds to tighter clusters.\n",
    "- Doesn't generally increase as more points are added (relative to Inertia)\n",
    "- When the similarity of points in the cluster is more important, you should use distortion\n",
    "\n",
    "So what can we do in order to find the clustering with best inertia? What we would do is, we initiate our K means algorithm several times. And with different initial configurations, and with that, assuming we predefined what our K is, we can compute the resulting inertia or distortion. Keep that results and see which one of our different initializations or configurations lead to the best inertia or distortion. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method and Applying K-means\n",
    "\n",
    "![](./images/03_ChoosingRightNumberOfCluster.png)\n",
    "\n",
    "### K-Means: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the clustering method.\n",
    "from sklearn. cluster import KMeans\n",
    "\n",
    "# Create an instance of the class.\n",
    "kmeans = KMeans(n_clusters = 3, init='k-means++')\n",
    "\n",
    "#Fit the instance on the data and then predict clusters for new data.\n",
    "kmeans = kmeans.fit(X1)\n",
    "y_predict = kmeans.predict(X2)\n",
    "\n",
    "# Can also be used in batch mode with MiniBatchKMeans.\n",
    "```\n",
    "\n",
    "### K-Means: Elbow Method Syntax\n",
    "\n",
    "To implement elbow method, fit K-Means for various levels of k, save inertia values.\n",
    "\n",
    "```python\n",
    "inertia = [ ]\n",
    "list_clusters = list(range(10))\n",
    "for k in list_clusters:\n",
    "  kmeans = KMeans(n_clusters=k)\n",
    "  kmeans.fit(X)\n",
    "  inertia.append(km.inertia_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

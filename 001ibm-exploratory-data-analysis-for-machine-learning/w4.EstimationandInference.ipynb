{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation and Inference, and Hypothesis Testing\n",
    "## Estimation and Inference\n",
    "- estimate is just going to give us an estimate of a certain parameter, such as the mean from our sample data.\n",
    "- When performing statistical inference, we're trying to understand the underlying distribution of the population, including our estimates of the mean, as well as other parameters such as the standard error of the underlying properties of the population that we're sampling from\n",
    "\n",
    "## Machine Learning and Statistical Inference\n",
    "Machine learning and statistical inference are similar\n",
    "\n",
    "(a case of computer science borrowing from a long history in statistics).\n",
    "\n",
    "In both cases, we're using data to learn/infer qualities of a distribution that generated the data\n",
    "(often termed the data -generating process).\n",
    "\n",
    "We may care either about the whole distribution or just features (e.g. mean).\n",
    "\n",
    "Machine learning applications that focus on understanding parameters and individual effects\n",
    "involve more tools from statistical inference (some applications are focused only on results).\n",
    "\n",
    "## Parametric versus Non-parametric. \n",
    "A statistical inference is about finding the underlying data generating process of our data, then the statistical model is going to be a set of the possible distributions or even reggressions that that data can take\n",
    "\n",
    "parametric model is a particular type of statistical model. What differentiates a parametric model? Some of the major characteristics, or that a parametric model is constrained to a finite number of parameters, and that'll rely on some strict assumptions made about the distributions from which that data is pulled.\n",
    "\n",
    "non-parametric models, will mean that our inference will not rely on as many assumptions, such as it will not have to rely on the data being pulled from a particular distribution, it'll be a distribution free inference.\n",
    "\n",
    "## common distributions\n",
    "- Uniform distribution\n",
    "- Normal/Gaussian distribution\n",
    "- Log-normal distribution => log transformation => normal distribution\n",
    "- exponential curve\n",
    "- poison distribution\n",
    "\n",
    "## Bayesian and frequentist statistics\n",
    "- Frequentist statistics is concerned with repeated observations to the limit\n",
    "- processes may have true frequencies in their real population mean or whatever it is. But we're interested in modeling probabilities as many, many repeats of an experiment\n",
    "\n",
    "## Frequentist vs. Bayesian: Bayesian\n",
    "A Bayesian describes parameters by probability distributions.\n",
    "\n",
    "Before seeing any data, a prior distribution (based on the experimenters' belief) is formulated.\n",
    "\n",
    "This prior distribution is then updated after seeing data (a sample from the distribution).\n",
    "\n",
    "After updating, the distribution is called the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hypothesis Testing\n",
    "A hypothesis is a statement about a population parameter.\n",
    "\n",
    "We create two hypotheses:\n",
    "- The null hypothesis (Ho)\n",
    "- The alternative hypothesis (H1, or HA)\n",
    "\n",
    "We decide which one to call the null depending on how the problem is set up.\n",
    "\n",
    "A hypothesis testing procedure gives us a rule to decide:\n",
    "- For which values of the test statistic do we accept Ho\n",
    "- For which values of the test statistic do we reject Ho, and accept H1,\n",
    "\n",
    "### Type 1 vs Type 2 Error\n",
    "\n",
    "- A type I error (false-positive) occurs if an investigator rejects a null hypothesis that is actually true in the population; \n",
    "- A type II error (false-negative) occurs if the investigator fails to reject a null hypothesis that is actually false in the population.\n",
    "\n",
    "### Hypothesis Testing Terminology\n",
    "\n",
    "- The likelihood ratio is called a test statistic: we use it to decide whether to accept/reject Ho-\n",
    "- The rejection region: is the set of values of the test statistic that lead to rejection of Ho-\n",
    "- The acceptance region: is the set of values of the test statistic that lead to acceptance of Ho-\n",
    "- The null distribution: is test statistic's distribution when the null is true.\n",
    "\n",
    "Hypothesis Testing: Marketing Intervention\n",
    "\n",
    "Testing marketing intervention effectiveness:\n",
    "- For a new direct mail marketing campaign to existing customers, the null\n",
    "hypothesis (Ho), suggests the campaign does not impact purchasing.\n",
    "- The alternative hypothesis (H,), suggests it has an impact.\n",
    "\n",
    "Hypothesis Testing: Product Quality/Size\n",
    "\n",
    "Testing whether a product meets expected size threshold:\n",
    "- Suppose a product is produced in various factories, with expected size S\n",
    "- To confirm that the product size meets the standard within a margin of error, the\n",
    "company might:\n",
    "- randomly sample from each production source,\n",
    "- establish Ho (product size is not significantly different from S),\n",
    "- and H, (there is a significant deviation in product size),\n",
    "- test whether Ho can be rejected in favor of H1, based on the observed mean\n",
    "and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance Level and P-Values\n",
    "\n",
    "Significance Level and P-Values\n",
    "- A significance level (α) is a probability threshold below which the null hypothesis\n",
    "will be rejected.\n",
    "- We must choose an a before computing the test statistic!\n",
    "- If we don't, we might be accused of p-hacking.\n",
    "- Choosing α is somewhat arbitrary, but often .01 or .05.\n",
    "\n",
    "Important terminology:\n",
    "- The p-value: smallest significance level at which the null hypothesis would be rejected.\n",
    "- The confidence interval: the values of the statistic for which we accept the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F Statistic\n",
    "\n",
    "Power: Bonferroni Correction\n",
    "- The Bonferroni Correction: says \"choose threshold\n",
    "SO that the probability of making a Type\n",
    "error (assuming no effect) is 5%\".\n",
    "\n",
    "Typically choose:\n",
    "- threshold = 0.05 / (# tests)\n",
    "- Bonferroni Correction allows the probability of a Type I error to be controlled,\n",
    "but at the cost of power.\n",
    "- Effects either need to be larger or the tests need larger samples, to be detected.\n",
    "- Best practice is to limit the number of comparisons done to a few well-motivated cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation vs Causation\n",
    "\n",
    "### How Correlations are Important\n",
    "We should be careful about changing X with the hope of changing Y.\n",
    "- X and Y can be correlated for different reasons:\n",
    "- X causes Y (what we want).\n",
    "- Y causes X (mixing up cause-and-effect).\n",
    "- X and Y are both caused by something else (confounding).\n",
    "- X and Y aren't really related, we just got unlucky in the sample (spurious).\n",
    "\n",
    "### Mixing Up Cause and Effect\n",
    "1. Student test scores are positively correlated with amount of time studied.\n",
    "\n",
    "This doesn't mean we should get students to study more by curving everyone's grades\n",
    "upward (this would likely have the opposite effect!). It is more likely that studying helps\n",
    "students learn material, so studying causes better performance.\n",
    "\n",
    "2. Customer satisfaction is negatively correlated with customer service call volume.\n",
    "\n",
    "This doesn't mean that we should remove or hide the customer service numbers,\n",
    "with the hope of improving customer satisfaction.\n",
    "\n",
    "\n",
    "### Confounding Variables\n",
    "Examples of confounding variables:\n",
    "\n",
    "1. The number of annual car accidents and the number of people named John are positively\n",
    "correlated (both are correlated with the population size).\n",
    "\n",
    "2. The amount of ice-cream sold and the number of drownings in a week are positively\n",
    "correlated (both are positively correlated with temperature).\n",
    "\n",
    "3. Number of factories a chip manufacturer owns and the number of chips sold are positively\n",
    "correlated (but both are driven by demand from the market).\n",
    "\n",
    "### Spurious Correlations\n",
    "These are correlations that are just \"coincidences\" due to the particular sample,\n",
    "and would probably not hold on longer samples / different samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary/Review\n",
    "### Estimation and Inference\n",
    "\n",
    "Inferential Statistics consist in learning characteristics of the population from a sample. The population characteristics are parameters, while the sample characteristics are statistics. A parametric model, uses a certain number of parameters like mean and standard deviation.\n",
    "\n",
    "The most common way of estimating parameters in a parametric model is through maximum likelihood estimation.\n",
    "\n",
    "Through a hypothesis test, you test for a specific value of the parameter.\n",
    "\n",
    "Estimation represents a process of determining a population parameter based on a model fitted to the data.\n",
    "\n",
    "The most common distribution functions are: uniform, normal, log normal, exponential, and poisson.\n",
    "\n",
    "A frequentist approach focuses in observing man repeats of an experiment. A bayesian approach describes parameters through probability distributions.\n",
    "\n",
    "### Hypothesis Testing\n",
    "A hypothesis is a statement about a population parameter. You commonly have two hypothesis: the null hypothesis and the alternative hypothesis.\n",
    "\n",
    "A hypothesis test gives you a rule to decide for which values of the test statistic you accept the null hypothesis and for which values you reject the null hypothesis and accept he alternative hypothesis.\n",
    "\n",
    "A type 1 error occurs when an effect is due to chance, but we find it to be significant in the model.\n",
    "\n",
    "A type 2 error occurs when we ascribe the effect to chance, but the effect is non-coincidental.\n",
    "\n",
    "### Significance level and p-values\n",
    "A significance level is a probability threshold below which the null hypothesis can be rejected. You must choose the significance level before computing the test statistic. It is usually .01 or .05.\n",
    "\n",
    "A p-value is the smallest significance level at which the null hypothesis would be rejected. The confidence interval contains the values of the statistic for which we accept the null hypothesis.\n",
    "\n",
    "Correlations are useful as effects can help predict an outcome, but correlation does not imply causation.\n",
    "\n",
    "When making recommendations, one should take into consideration confounding variables and the fact that correlation across two variables do not imply that an increase or decrease in one of them will drive an increase or decrease of the other.\n",
    "\n",
    "Spurious correlations happen in data. They are just coincidences given a particular data sample."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "This module will walk you through the main idea of how support vector machines construct hyperplanes to map your data into regions that concentrate a majority of data points of a certain class. Although support vector machines are widely used for regression, outlier detection, and classification, this module will focus on the latter.\n",
    "\n",
    "## Learning Objectives\n",
    "- Identify common supervised machine learning algorithms.\n",
    "- Describe and use support vector machines for classification\n",
    "- Realize the importance of the kernel trick for non-linear classification\n",
    "- Build support vector machines models with sklearn\n",
    "\n",
    "## Introduction to Support Vector Machines\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "- Support Vector Machines (SVM) approach to Classification\n",
    "- Comparing SVM and Logistic Regression\n",
    "- The SVM cost function\n",
    "- Regularization in SVM models\n",
    "\n",
    "![](./images/21_SupportVectorMachines.png)\n",
    "\n",
    " the blue and red samples that define that margin, those dotted lines, are going to be called support vectors. So those are the support vectors for our support vector machine.\n",
    "\n",
    "\n",
    "## Classification with Support Vector Machines\n",
    "\n",
    "![](./images/22_SVMsClassification.png) \n",
    "\n",
    "\n",
    "## The Support Vector Machines Cost Function\n",
    "\n",
    "![](./images/23_SVMs.png)\n",
    "\n",
    "![](./images/24_SVMsCostFunction.png)\n",
    "\n",
    "\n",
    "![](./images/25_OutlierSensitivity.png)\n",
    "\n",
    "\n",
    "## Regularization in Support Vector Machines\n",
    "\n",
    "![](./images/26_RegularizationSVMs.png)\n",
    "\n",
    "$$\n",
    "    J(\\beta_i) = SVMCost(\\beta_i) + \\dfrac{1}{C}\\Sigma_i \\beta_i\n",
    "$$\n",
    "\n",
    "![](./images/27_Interpretation.png)\n",
    "\n",
    "## Linear SVM: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. svm import LinearSVC\n",
    "# Create an instance of the class\n",
    "LinSVC = LinearSVC (penalty='12', C=10.0) # regularization parameters\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "LinSVC = LinSVC. fit ( (X train, y train)\n",
    "y_predict = LinSVC. predict (X test)\n",
    "\n",
    "# Tune regularization parameters with cross-validation.\n",
    "# Use LinearSVM for regression.\n",
    "```\n",
    "\n",
    "# Support Vector Machines Kernels\n",
    "\n",
    "In this section, we will cover:\n",
    "- Using kernels with Support Vector Machines (SVM)\n",
    "- Non-linear decision boundaries\n",
    "- Implementation of SVM kernel modeling\n",
    "\n",
    "![](./images/28_NonLinearToLinear.png)\n",
    "\n",
    "![](./images/29_KernelTrick.png)\n",
    "\n",
    "\n",
    "## Introduction to Support Vector Machines Gaussian Kernels\n",
    "\n",
    "![](./images/30_SVMGaussianKernel.png)\n",
    "\n",
    "Approach 1:\n",
    "- Create higher order features\n",
    "to transform the data.\n",
    "$$\n",
    "Budget^2 + Rating^2 +\n",
    "Budget * Rating +\n",
    "$$\n",
    "\n",
    "![](./images/31_SVMGaussianKernel.png)\n",
    "\n",
    "Approach 2:\n",
    "- Transform the space to a different coordinate system\n",
    "- Define new Feature: \n",
    "  - Feature 1: Similarity to \"Pulp fiction\"\n",
    "  - Feature 2: Similarity to \"Black swan\"\n",
    "  - Feature 3: Similarity to \"Transformer\"\n",
    "- Create Gaussian function at new features\n",
    "$$\n",
    "a_1(x^{obs})=exp[\\dfrac{-\\sum(x_i^{obs}-x_i^{new feature})^2}{2\\sigma^2}]\n",
    "$$\n",
    "\n",
    "### Transformation\n",
    "![](./images/32_SVmGaussionaKernel.png)\n",
    "\n",
    "### Classification\n",
    "![](./images/33_SVmGaussionaKernel.png)\n",
    "\n",
    "\n",
    "## Support Vector Machines Gaussian Kernels\n",
    "\n",
    "### Syntax\n",
    "\n",
    "SVMs with Kernels: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. svm import SVC\n",
    "# Create an instance of the class\n",
    "\n",
    "rbfSVC = SVC (kernel=' rbf', gamma=1.0, C=10.0) # \"C\" is penalty associated with the error term\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "rbfSVC.fit(X_train, Y_train)\n",
    "y_predict = rbfSVC .predict (X_test)\n",
    "\n",
    "# Tune kernel and associated parameters with cross-validation.\n",
    "\n",
    "```\n",
    "\n",
    "## Support Vector Machines Workflow\n",
    "\n",
    "![](./images/34_SVMsWorkflow.png)\n",
    "\n",
    "![](./images/35_ModelChoice.png)\n",
    "\n",
    "## Implementing Support Vector Machines Kernal Models\n",
    "\n",
    "Faster Kernel Transformations: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. kernel approximation import Nystroem\n",
    "# Create an instance of the class\n",
    "NystroemSVC = Nystroem (kernel= 'rbf', gamma=1.0,n_components=100) # n _components is number of samples\n",
    "\n",
    "# Fit the instance on the data and transform\n",
    "X train = NystroemSVC. fit transform (X train)\n",
    "X test = NystroemSVC. transform (X test)\n",
    "```\n",
    "\n",
    "Tune kernel and associated parameters with cross-validation.\n",
    "\n",
    "\n",
    "Faster Kernel Transformations: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. kernel approximation import RBFsampler\n",
    "# Create an instance of the class\n",
    "rbfSample = RBFsampler (gamma=1.0, n_components=100)\n",
    "\n",
    "# Fit the instance on the data and transform\n",
    "X_train = rbfSample.fit transform (X_train)\n",
    "X_test = rbfSample.transform(X_test)\n",
    "```\n",
    "Tune kernel parameters and components with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Review\n",
    "The main idea behind support vector machines is to find a hyperplane that separates classes by determining decision boundaries that maximize the distance between classes.\n",
    "\n",
    "When comparing logistic regression and SVMs, one of the main differences is that the cost function for logistic regression has a cost function that decreases to zero, but rarely reaches zero. SVMs use the Hinge Loss function as a cost function to penalize misclassification. This tends to lead to better accuracy at the cost of having less sensitivity on the predicted probabilities.\n",
    "\n",
    "Regularization can help SVMs generalize better with future data.\n",
    "\n",
    "By using gaussian kernels, you transform your data space vectors into a different coordinate system, and may have better chances of finding a hyperplane that classifies well your data.SVMs with RBFs Kernels are slow to train with data sets that are large or have many features.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision tree methods are a common baseline model for classification tasks due to their visual appeal and high interpretability. This module walks you through the theory behind decision trees and a few hands-on examples of building decision tree models for classification. You will realize the main pros and cons of these techniques. This background will be useful when you are presented with decision tree ensembles in the next module.\n",
    "\n",
    "## Learning Objectives\n",
    "- Describe and use decision trees and decision-tree ensemble models for classification\n",
    "- Identify and implement common ensemble models for classification, including bagging, boosting, stacking, and random forest.\n",
    "- Become familiarized with the pros and cons of decision tree methods\n",
    "- Build decision trees models with sklearn\n",
    "\n",
    "## Overview of Classifiers\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "- Overview of Classification problems\n",
    "- Decision Tree Classification algorithm\n",
    "- Splitting Decision Trees: entropy and information gain\n",
    "- Pruning Decision Trees to address overfitting\n",
    "\n",
    "## Introduction to Decision Trees\n",
    "\n",
    "![](./images/36_DecisionTree.png)\n",
    "\n",
    "![](./images/37_DCContinuousValues.png)\n",
    "\n",
    "![](./images/38_DCCV_depth.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Decision Tree\n",
    "\n",
    "![](./images/39_DCDepth.png)\n",
    "\n",
    "![](./images/40_bestDC.png)\n",
    "\n",
    "![](./images/41_Spliting.png)\n",
    "\n",
    "$$\n",
    "E(t) = 1 - max_i[p(i|t)]\n",
    "$$\n",
    "\n",
    "## Entropy-based Splitting\n",
    "\n",
    "![](./images/43_EntropySpliting.png)\n",
    "\n",
    "$$\n",
    "H(t) = - \\sum{i=1}{n}p(i|t)log_2[p(i|t)]\n",
    "$$\n",
    "\n",
    "![](./images/44_EntropyErrorFunction.png)\n",
    "\n",
    "We can then take the weighted average of each as we did with that classification error, and we see that we have decreased entropy by 0.0441. So here, rather than the entropy being exactly the same, when we take the weighted average according to how much of the subset from our parent node, one into each one of the child nodes, we see that the entropy has decreased by 0.0441.\n",
    "\n",
    "![](./images/45_EntropySumary.png)\n",
    "\n",
    "\n",
    "## Other Decision Tree Splitting Criteria\n",
    "\n",
    "![](./images/46_ClassificationErrorVsEntropy.png)\n",
    "\n",
    "![](./images/47_vsEntropy.png)\n",
    "\n",
    "![](./images/48_InfomationGain.png)\n",
    "\n",
    "![](./images/49_GiniIndex.png)\n",
    "\n",
    "\n",
    "## Pros and Cons of Decision Trees\n",
    "\n",
    "![](./images/50_HighVariance.png)\n",
    "\n",
    "![](./images/51_Strengths.png)\n",
    "\n",
    "## Decision TreeClassifier: The Syntax\n",
    "```python\n",
    "\n",
    "# Import the class containing the classification method\n",
    "from sklearn. tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "DTC = DecisionTreeClassifier (criterion= 'Gini', max features=10, max depth=5) # tree parameters\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "DTC = DTC. fit (X train, y train)\n",
    "y_predict = DTC. predict (X test)\n",
    "\n",
    "#Tune parameters with cross-validation. Use DecisionTreeRegressor for regression.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Review\n",
    "\n",
    "Decision trees split your data using impurity measures. They are a greedy algorithm and are not based on statistical assumptions.\n",
    "\n",
    "The most common splitting impurity measures are Entropy and Gini index.Decision trees tend to overfit and to be very sensitive to different data.\n",
    "\n",
    "Cross validation and pruning sometimes help with some of this.\n",
    "\n",
    "Great advantages of decision trees are that they are really easy to interpret and require no data preprocessing.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

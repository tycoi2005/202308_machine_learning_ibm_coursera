{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Unbalanced Classes\n",
    "\n",
    "Some classification models are better suited than others to outliers, low occurrence of a class, or rare events. The most common methods to add robustness to a classifier are related to stratified sampling to re-balance the training data. This module will walk you through both stratified sampling methods and more novel approaches to model data sets with unbalanced classes. \n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Identify class weights and sampling as methods to deal with unbalanced classes in a data set.\n",
    "\n",
    "Recognize the syntax for building for sampling, blagging, and nearest neighbor methods for modeling unbalanced classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretability\n",
    "\n",
    "In general, we need explanation methods that can make the behaviors and predictions of machine learning models understandable to humans. We need to use those methods to understand the model structures, what important features should be included in the model, and how those models map features to prediction outcomes.\n",
    "\n",
    "\n",
    "In addition, sometimes knowing how models work exactly may give us more insights than merely predicting the outcomes. For example, understanding how an AI system diagnoses cancer may help human health experts identify evidence-based risk factors. For decision makers, interpretability is important especially for those in very sensitive or high-risk domains such as finance or health. We need to be confident and be able to trust that the model is working correctly. Black-box machine learning systems cannot be trusted unless they can be monitored and interpreted. As such, building trustable models is sometimes even more important than building high-performing models.\n",
    "\n",
    "Understanding machine learning models for:\n",
    "- Model explaination\n",
    "- Model trust\n",
    "- Model debug \n",
    "\n",
    "Recap\n",
    "- We can only trust and effectively debug machine\n",
    "learning models if they are understandable\n",
    "- Self-interpretable models have simple and\n",
    "intuitive structures\n",
    "- Non- self- interpretable models have complex\n",
    "structures and can be described as black-box\n",
    "models\n",
    "\n",
    "### Examples of Self-Interpretable and Non-Self-Interpretable Models\n",
    "\n",
    "#### Self-Interpretable\n",
    "\n",
    "Linear models are probably the most widely used predictive models due to their simplicity and effectiveness, especially in the financial industry. Their structure is simple with just a linear combination of features that predict values. As such, linear model prediction outcomes often require minimal effort to understand.\n",
    "\n",
    "Tree models such as decision trees, are another popular self-interpretable type of model. The main characteristic of tree models is they mimic humanâ€™s reasoning process via creating a set of IF-THEN-ELSE rules. \n",
    "\n",
    "The K-nearest neighbor model, or KNN, can also be considered a self-interpretable model if the feature spaces can be comprehensible and kept small.\n",
    "\n",
    "#### Non-Self-Interpretable Models\n",
    "\n",
    "Ensemble Models\n",
    "\n",
    "![](./images/70_ModelInterpretationMethods.png)\n",
    "\n",
    "\n",
    "\n",
    "### Model-Agnostic Explanations\n",
    "\n",
    "![](./images/71_ModelAgnosticExplainations.png)\n",
    "\n",
    "Feature importance\n",
    "\n",
    "Measure the importance of features\n",
    "\n",
    "1. Simplify your model by only including important features\n",
    "2. Interpret how predictions were made\n",
    "\n",
    "Permutation feature importance\n",
    "- The basic idea of permutation feature importance is very simple. For each feature, we shuffle its feature values and use the model to make predictions based on the shuffled values. In most cases, the prediction error will increase. Permuting important or impactful features will tend to generate large prediction errors and less important features will tend to generate small error increases. As such, feature importance can be measured by calculating the difference between the prediction errors before and after permutation. \n",
    "\n",
    "![](./images/72_PermutationFeatureImportanceExample.png)\n",
    "\n",
    "- Partial Dependency Plot is an effective way to illustrate the relationship between a feature and the model outcome. It essentially visualizes the marginal effects of a feature, that is, it shows how the model outcome changes when a specific feature changes in its distribution. Note that we keep the rest of the features unchanged while changing the interested feature. \n",
    "\n",
    "Impurity-based feature importance\n",
    "\n",
    "Shapley Additive exPlanations (SHAP) values\n",
    "\n",
    "### Surrogate Models\n",
    "\n",
    "![](./images/73_SurogateModels.png)\n",
    "\n",
    "![](./images/74_GlobalSurrogateModels.png)\n",
    "\n",
    "Local surrogate\n",
    "- Global surrogate models may not always work\n",
    "  - Large inconsistency between surrogate models and black-box models\n",
    "  - Multiple data instance groups or clusters in the dataset\n",
    "- Explain specific interested data instances locally\n",
    "- A local surrogate model is built on one or a few instances\n",
    "\n",
    "Local Interpretable Model-Agnostic Explanations (LIME)\n",
    "![](./images/75_LocalInterpretableModel-AgnosticExplanations.png)\n",
    "\n",
    "\n",
    "### Practice Lab: Model Interpretability\n",
    "\n",
    "### Practice: Model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Unbalanced Classes\n",
    "\n",
    "### Upsampling and Downsampling\n",
    "\n",
    "### Modeling Approaches: Weighting and Stratified Sampling\n",
    "\n",
    "### Modeling Approaches: Random and Synthetic Oversampling\n",
    "\n",
    "### Modeling Approaches: Nearing Neighbor Methods\n",
    "\n",
    "### Modeling Approaches: Blagging"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

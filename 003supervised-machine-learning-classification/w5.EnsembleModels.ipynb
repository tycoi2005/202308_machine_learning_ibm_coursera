{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models\n",
    "\n",
    "Ensemble models are a very popular technique as they can assist your models be more resistant to outliers and have better chances at generalizing with future data. They also gained popularity after several ensembles helped people win prediction competitions. Recently, stochastic gradient boosting became a go-to candidate model for many data scientists. This model walks you through the theory behind ensemble models and popular tree-based ensembles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Identify, use, and interpret common ensemble models for classification, including bagging, boosting, stacking, and random forest.\n",
    "\n",
    "Build ensemble models with sklearn, including bagging, boosting, stacking, and random forest.\n",
    "\n",
    "Identify common supervised machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "\n",
    "- Combining models (ensemble-based methods)\n",
    "- Bootstrap aggregation (Bagging)\n",
    "- Bagging, Random Forest, and Extra Trees Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Based Methods and Bagging\n",
    "\n",
    "![](./images/52_CombineModelPredictions.png)\n",
    "\n",
    "![](./images/53_AggregateResults.png)\n",
    "\n",
    "### Bagging = Bootstrap Aggregating\n",
    "\n",
    "![](./images/55_HowManyTRees.png)\n",
    "\n",
    "![](./images/56_BaggingErrorCalculations.png)\n",
    "\n",
    "### Bagging Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "BC = BaggingClassifier (n estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "BC = BC. fit (X train, y train)\n",
    "y_predict = BC. predict (X test)\n",
    "\n",
    "```\n",
    "Tune parameters with cross-validation. Use BaggingRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "![](./images/57_ReductionInVarianceDueToBagging.png)\n",
    "\n",
    "![](./images/58_RandomForest.png)\n",
    "\n",
    "![](./images/59_HowManyTreesInForest.png)\n",
    "\n",
    "### RandomForest: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "RC = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "RC = RC.fit(X_train, y_train)\n",
    "y_predict = RC.predict (X_test)\n",
    "```\n",
    "\n",
    "Tune parameters with cross-validation. Use RandomForestRegressor for regression.\n",
    "\n",
    "\n",
    "### Introducing Even More Randomness\n",
    "\n",
    "Sometimes additional randomness is desired beyond Random Forest.\n",
    "\n",
    "Solution: select features randomly and create splits randomly --- don't choose greedily.\n",
    "\n",
    "Called \"Extra Random Trees\".\n",
    "\n",
    "### Extra Trees Classifier: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "EC = ExtraTreesClassifier (n estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "EC = EC. fit (X train, y train)\n",
    "y_ predict = EC. predict (X test)\n",
    "```\n",
    "\n",
    "Tune parameters with cross-validation. Use ExtraTreesRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting and stacking\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "- The Boosting approach to combining models\n",
    "- Types of Boosting models: Gradient Boosting, AdaBoost\n",
    "- Boosting loss functions\n",
    "- Combining heterogeneous classifiers\n",
    "\n",
    "![](./images/60_BaggingReview.png)\n",
    "\n",
    "![](./images/61_Boosting.png)\n",
    "\n",
    "![](./images/62_Boosting.png)\n",
    "\n",
    "\n",
    "### Adaboost and Gradient Boosting Overview\n",
    "\n",
    "![](./images/63_BoostingSpecifics.png)\n",
    "\n",
    "![](./images/64_01LossFunction.png)\n",
    "\n",
    "![](./images/65_AdaBoostLossFunction.png)\n",
    "\n",
    "![](./images/66_GradientBoostingLossFunction.png)\n",
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "|       Bagging                    |  Boosting                          |\n",
    "|----------------------------------|------------------------------------|\n",
    "| Bootstrapped samples             | Fit entire data set                |\n",
    "| Base trees created independently | Base trees created successively    |\n",
    "| Only data points considered      | Use residuals from previous models |\n",
    "| No weighting used                | Up-weight misclassified points     |\n",
    "| Excess trees will not overfit    | Beware of overfitting              |\n",
    "\n",
    "![](./images/67_TurningGradientBoostedModel.png)\n",
    "\n",
    "![](./images/67_TurningGradientBoostedModel2.png)\n",
    "\n",
    "### GradientBoosting Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "GBC = GradientBoostingClassifier (learning_rate=0.1,\n",
    "max features=1, subsample=0.5,\n",
    "n estimators=200)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "GBC = GBC. fit (X train, y train)\n",
    "y predict = GBC. predict (X test)\n",
    "```\n",
    "\n",
    "Tune with cross-validation. Use GradientBoostingRegressor for regression.\n",
    "\n",
    "\n",
    "### AdaBoostClassifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import AdaBoostClassifier\n",
    "from sklearn. tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "\n",
    "ABC = AdaBoostClassifier( base_estimator= DecisionTreeClassifier(),learning_rate=0.1, n_estimators=200) # can also set max depth here\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "ABC = ABC.fit (X_train, y_train)\n",
    "Y_predict = ABC.predict (X_test)\n",
    "\n",
    "```\n",
    "Tune with cross-validation. Use AdaBoostRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "![](./images/69_Stacking_CombiningClassifiers.png)\n",
    "\n",
    "- Output of base learners can\n",
    "be combined via majority vote\n",
    "or weighted.\n",
    "- Additional hold-out data needed\n",
    "if meta learner parameters are\n",
    "used.\n",
    "- Be aware of increasing\n",
    "model complexity.\n",
    "- The final prediction can be done\n",
    "by voting or with another model\n",
    "\n",
    "### Voting Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import VotingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "VC = VotingClassifier (estimator_list) # estimator_list is list of modal were fitted already\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "VC = VC. fit (X train, y train)\n",
    "Y_predict = VC. predict (X test)\n",
    "# Use VotingRegressor for regression.\n",
    "# The StackingClassifier (or StackingRegressor) works similarly:\n",
    "SC = StackingClassifier (estimator list, final estimator=LogisticRegression ())\n",
    "\n",
    "```\n",
    "\n",
    "### Learning Recap\n",
    "\n",
    "In this section, we discussed:\n",
    "- The Boosting approach to combining models\n",
    "- Types of Boosting models: Gradient Boosting, AdaBoost\n",
    "- Boosting loss functions\n",
    "- Combining heterogeneous classifiers\n",
    "\n",
    "Further reading:\n",
    "XGBoost is another popular boosting algorithm (not in Scikit-Learn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Review\n",
    "\n",
    "## Ensemble Based Methods and Bagging\n",
    "\n",
    "Tree ensembles have been found to generalize well when scoring new data. Some useful and popular tree ensembles are bagging, boosting, and random forests. Bagging, which combines decision trees by using bootstrap aggregated samples. An advantage specific to bagging is that this method can be multithreaded or computed in parallel. Most of these ensembles are assessed using out-of-bag error.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Random forest is a tree ensemble that has a similar approach to bagging. Their main characteristic is that they add randomness by only using a subset of features to train each split of the trees it trains. Extra Random Trees is an implementation that adds randomness by creating splits at random, instead of using a greedy search to find split variables and split points.\n",
    "\n",
    "## Boosting\n",
    "Boosting methods are additive in the sense that they sequentially retrain decision trees using the observations with the highest residuals on the previous tree. To do so, observations with a high residual are assigned a higher weight.\n",
    "\n",
    "## Gradient Boosting\n",
    "The main loss functions for boosting algorithms are:\n",
    "\n",
    "0-1 loss function, which ignores observations that were correctly classified. The shape of this loss function makes it difficult to optimize.\n",
    "\n",
    "Adaptive boosting loss function, which has an exponential nature. The shape of this function is more sensitive to outliers.\n",
    "\n",
    "Gradient boosting loss function. The most common gradient boosting implementation uses a binomial log-likelihood loss function called deviance. It tends to be more robust to outliers than AdaBoost.\n",
    "\n",
    "The additive nature of gradient boosting makes it prone to overfitting. This can be addressed using cross validation or fine tuning the number of boosting iterations. Other hyperparameters to fine tune are:\n",
    "\n",
    "learning rate (shrinkage)\n",
    "\n",
    "subsample\n",
    "\n",
    "number of features.\n",
    "\n",
    "## Stacking\n",
    "Stacking is an ensemble method that combines any type of model by combining the predicted probabilities of classes. In that sense, it is a generalized case of bagging. The two most common ways to combine the predicted probabilities in stacking are: using a majority vote or using weights for each predicted probability.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

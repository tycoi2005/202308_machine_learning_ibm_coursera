{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble models\n",
    "\n",
    "Ensemble models are a very popular technique as they can assist your models be more resistant to outliers and have better chances at generalizing with future data. They also gained popularity after several ensembles helped people win prediction competitions. Recently, stochastic gradient boosting became a go-to candidate model for many data scientists. This model walks you through the theory behind ensemble models and popular tree-based ensembles.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Identify, use, and interpret common ensemble models for classification, including bagging, boosting, stacking, and random forest.\n",
    "\n",
    "Build ensemble models with sklearn, including bagging, boosting, stacking, and random forest.\n",
    "\n",
    "Identify common supervised machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "\n",
    "- Combining models (ensemble-based methods)\n",
    "- Bootstrap aggregation (Bagging)\n",
    "- Bagging, Random Forest, and Extra Trees Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Based Methods and Bagging\n",
    "\n",
    "![](./images/52_CombineModelPredictions.png)\n",
    "\n",
    "![](./images/53_AggregateResults.png)\n",
    "\n",
    "### Bagging = Bootstrap Aggregating\n",
    "\n",
    "![](./images/55_HowManyTRees.png)\n",
    "\n",
    "![](./images/56_BaggingErrorCalculations.png)\n",
    "\n",
    "### Bagging Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "BC = BaggingClassifier (n estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "BC = BC. fit (X train, y train)\n",
    "y_predict = BC. predict (X test)\n",
    "\n",
    "```\n",
    "Tune parameters with cross-validation. Use BaggingRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "![](./images/57_ReductionInVarianceDueToBagging.png)\n",
    "\n",
    "![](./images/58_RandomForest.png)\n",
    "\n",
    "![](./images/59_HowManyTreesInForest.png)\n",
    "\n",
    "### RandomForest: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "RC = RandomForestClassifier(n_estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "RC = RC.fit(X_train, y_train)\n",
    "y_predict = RC.predict (X_test)\n",
    "```\n",
    "\n",
    "Tune parameters with cross-validation. Use RandomForestRegressor for regression.\n",
    "\n",
    "\n",
    "### Introducing Even More Randomness\n",
    "\n",
    "Sometimes additional randomness is desired beyond Random Forest.\n",
    "\n",
    "Solution: select features randomly and create splits randomly --- don't choose greedily.\n",
    "\n",
    "Called \"Extra Random Trees\".\n",
    "\n",
    "### Extra Trees Classifier: The Syntax\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "EC = ExtraTreesClassifier (n estimators=50)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "EC = EC. fit (X train, y train)\n",
    "y_ predict = EC. predict (X test)\n",
    "```\n",
    "\n",
    "Tune parameters with cross-validation. Use ExtraTreesRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting and stacking\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "- The Boosting approach to combining models\n",
    "- Types of Boosting models: Gradient Boosting, AdaBoost\n",
    "- Boosting loss functions\n",
    "- Combining heterogeneous classifiers\n",
    "\n",
    "![](./images/60_BaggingReview.png)\n",
    "\n",
    "![](./images/61_Boosting.png)\n",
    "\n",
    "![](./images/62_Boosting.png)\n",
    "\n",
    "\n",
    "### Adaboost and Gradient Boosting Overview\n",
    "\n",
    "![](./images/63_BoostingSpecifics.png)\n",
    "\n",
    "![](./images/64_01LossFunction.png)\n",
    "\n",
    "![](./images/65_AdaBoostLossFunction.png)\n",
    "\n",
    "![](./images/66_GradientBoostingLossFunction.png)\n",
    "\n",
    "### Bagging vs Boosting\n",
    "\n",
    "|       Bagging                    |  Boosting                          |\n",
    "|----------------------------------|------------------------------------|\n",
    "| Bootstrapped samples             | Fit entire data set                |\n",
    "| Base trees created independently | Base trees created successively    |\n",
    "| Only data points considered      | Use residuals from previous models |\n",
    "| No weighting used                | Up-weight misclassified points     |\n",
    "| Excess trees will not overfit    | Beware of overfitting              |\n",
    "\n",
    "![](./images/67_TurningGradientBoostedModel.png)\n",
    "\n",
    "![](./images/67_TurningGradientBoostedModel2.png)\n",
    "\n",
    "### GradientBoosting Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "GBC = GradientBoostingClassifier (learning_rate=0.1,\n",
    "max features=1, subsample=0.5,\n",
    "n estimators=200)\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "GBC = GBC. fit (X train, y train)\n",
    "y predict = GBC. predict (X test)\n",
    "```\n",
    "\n",
    "Tune with cross-validation. Use GradientBoostingRegressor for regression.\n",
    "\n",
    "\n",
    "### AdaBoostClassifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import AdaBoostClassifier\n",
    "from sklearn. tree import DecisionTreeClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "\n",
    "ABC = AdaBoostClassifier( base_estimator= DecisionTreeClassifier(),learning_rate=0.1, n_estimators=200) # can also set max depth here\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "ABC = ABC.fit (X_train, y_train)\n",
    "Y_predict = ABC.predict (X_test)\n",
    "\n",
    "```\n",
    "Tune with cross-validation. Use AdaBoostRegressor for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "![](./images/69_Stacking_CombiningClassifiers.png)\n",
    "\n",
    "- Output of base learners can\n",
    "be combined via majority vote\n",
    "or weighted.\n",
    "- Additional hold-out data needed\n",
    "if meta learner parameters are\n",
    "used.\n",
    "- Be aware of increasing\n",
    "model complexity.\n",
    "- The final prediction can be done\n",
    "by voting or with another model\n",
    "\n",
    "### Voting Classifier: The Syntax\n",
    "\n",
    "```python\n",
    "# Import the class containing the classification method\n",
    "from sklearn. ensemble import VotingClassifier\n",
    "\n",
    "# Create an instance of the class\n",
    "VC = VotingClassifier (estimator_list) # estimator_list is list of modal were fitted already\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "VC = VC. fit (X train, y train)\n",
    "Y_predict = VC. predict (X test)\n",
    "# Use VotingRegressor for regression.\n",
    "# The StackingClassifier (or StackingRegressor) works similarly:\n",
    "SC = StackingClassifier (estimator list, final estimator=LogisticRegression ())\n",
    "\n",
    "```\n",
    "\n",
    "### Learning Recap\n",
    "\n",
    "In this section, we discussed:\n",
    "- The Boosting approach to combining models\n",
    "- Types of Boosting models: Gradient Boosting, AdaBoost\n",
    "- Boosting loss functions\n",
    "- Combining heterogeneous classifiers\n",
    "\n",
    "Further reading:\n",
    "XGBoost is another popular boosting algorithm (not in Scikit-Learn).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to supervised leaning and linear regression\n",
    "## Basic Knowledge\n",
    "In order to be successful in this course, you will need a working knowledge of the following:\n",
    "\n",
    "- Familiarity with programming on a Python development environment\n",
    "\n",
    "- Familiarity with Jupyter notebooks\n",
    "\n",
    "- Fundamental understanding of Calculus, Linear Algebra, Probability, and Statistics\n",
    "\n",
    "- Familiarity with Exploratory Data Analysis, Feature Engineering, handling missing values, and handling categorical values\n",
    "\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "A model is a small thing that captures a larger thing.\n",
    "\n",
    "A good model is going to omit unimportant details while retaining what's important. \n",
    "\n",
    "A map is a model of the world \n",
    "\n",
    "\n",
    "## Fit parameters and hyperparameters\n",
    "\n",
    "$$\n",
    "y_p = f(\\Omega, x)\n",
    "$$\n",
    "\n",
    "Our framework estimates a relationship between the features and target:\n",
    "\n",
    "Here, N (the Fit Parameters) involve aspects of the model we estimate (fit) using the data.\n",
    "\n",
    "To implement our approach, we make decisions regarding how to produce these estimates.\n",
    "\n",
    "These decisions lead to hyperparameters, that are an important part of the machine learning\n",
    "workflow (though not explicit components of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main modeling approaches:\n",
    "- Regression: y is numeric.\n",
    "\n",
    "  - E.g.: stock price, box office revenue, location (x,y coordinates).\n",
    "\n",
    "- Classification: y is categorical.\n",
    "\n",
    "  - E.g.: face recognition customer churn, which word comes next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_p = f(\\Omega, x)\n",
    "$$\n",
    "\n",
    "\n",
    "x     : Input.\n",
    "\n",
    "$y_p$ : Output (values predicted by the model).\n",
    "\n",
    "f(:)  : Prediction function that generates predictions from x and 0.\n",
    "\n",
    "\n",
    "\n",
    "Data scientists will train the model to find the best parameters by looking at past data.\n",
    "\n",
    "\n",
    "J (Y, $y_p$): Loss\n",
    "\n",
    "Most ML models define a quantitative score for how \"good\" our predictions are.\n",
    "\n",
    "Typically measures how close our predictions are to the true values.\n",
    "\n",
    "Update rule:  determine how to update our parameters, typically trying to find those parameters that will minimize that loss function J . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "\n",
    "### Interpretation and Prediction\n",
    "\n",
    "Interpretation:\n",
    "- In some cases, the primary objective is to train a model to find insights from the data.\n",
    "- In $ y_p = f(\\Omega, x) $, the interpretation approach uses N to give us insight into a system.\n",
    "Common workflow:\n",
    "- Gather x, y; Train model by finding the N that gives the best prediction $ y_p = f(\\Omega, x) $.\n",
    "- Focus on $\\Omega$ (rather than $y_p$) to generate insights.\n",
    "\n",
    "Example interpretation exercises:\n",
    "- X = customer demographics, y = sales data; examine N to understand loyalty by segment\n",
    "- x = car safety features, y = traffic accidents; examine N to understand what makes cars safer\n",
    "- x = marketing budget, y = movie revenue; examine 2 to understand marketing effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction:\n",
    "- In some cases, the primary objective is to make the best prediction.\n",
    "- In $ y_p = f(\\Omega, x) $, the prediction approach compares $y_p$ with y.\n",
    "- The focus is on performance metrics, which measure the quality of the model's predictions.\n",
    "- Performance metrics usually involve some measure of closeness between $y_p$ with y.\n",
    "- Without focusing on interpretability, we risk having a Black-box model.\n",
    "\n",
    "Example prediction exercises:\n",
    "- $x$ = customer purchase history , y = customer churn; focus on predicting customer churn\n",
    "- $x$ = financial information, y = flagged default/non-default; focus on predicting loan default\n",
    "- $x$ = purchase history, y = next purchase; focus on predicting the next purchase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Common Approaches\n",
    "\n",
    "Interpretation and prediction in Supervised Machine Learning\n",
    "\n",
    "Majority of projects will call for a balance.\n",
    "\n",
    "Interpretation can provide insight into improvements in prediction, and vice-versa.\n",
    "\n",
    "Not all models will allow both: Supervised Machine Learning models provide\n",
    "\n",
    "varying levels of support for interpretation vs. prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Framework: Takeaways\n",
    "\n",
    "Machine Learning is the subset of Al that focuses on model building to support a goal of\n",
    "interpretation and/or prediction.\n",
    "\n",
    "ML algorithms:\n",
    "- Use past experience to build a model that is useful for future experience.\n",
    "- Follow a general form: $y_p = f(\\Omega,x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning overview\n",
    "\n",
    "![Supervised learning overview](./images/01_SupervisedLearningOverview.jpg \"Supervised learning overview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Numeric predicting: Movie Revenue](./images/02_NumericPredicting_MovieRevenue.png \"Numeric predicting: Movie Revenue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Classification: Category Answers](./images/03_ClassificationCategoricalAnswers.png \"Classification: Category Answers\")\n",
    "\n",
    "\n",
    "![Classification: Category Answers Examples](./images/04_ClassificationCategoricalAnswers.png \"Classification: Category Answers Example\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Needed for Classification?\n",
    "\n",
    "Model data with:\n",
    "- Features that can be quantified\n",
    "- Labels that are known\n",
    "- Method to measure similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "![Calculating The residuals](./images/05_CalculatingTheResiduals.png \"Caculating the Residuals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing the Error Function\n",
    "\n",
    "$$\n",
    "J(\\beta_0,\\beta_1)=\\dfrac{1}{2m}\\sum_{i=1}^m((\\beta_0+\\beta_1 x_{obs}^i)-y_{obs}^i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Best Practice\n",
    "\n",
    "- Use cost function to fit model\n",
    "\n",
    "- Develop multiple models\n",
    "\n",
    "- Compare results and choose best one\n",
    "\n",
    "![Other Measures Of Error](./images/06_OtherMeasuresOfError.png \"Other Measures Of Error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSE: sum squared error. \n",
    "\n",
    "TSS: Total Squared Error\n",
    "\n",
    "That's going to measure the distance between truth and our predictions, similar to that portion of our cost function that we saw earlier. That's going to be sum of squared error. Our total squared error just measures the distance between the truth and the average values of the truth. Sum of squared error is the unexplained variation from our model. We had a line through each one of our dots and it's going to be, what were we not able to explain. That's going to be the sum of squared error and then the total squared error is the total variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression: The Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(2, 0)) while a minimum of 1 is required by LinearRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m LR \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m      7\u001b[0m \u001b[39m# Fit the instance on the data and then predict the expected value\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m LR \u001b[39m=\u001b[39m LR\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      9\u001b[0m y_predict \u001b[39m=\u001b[39m LR\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_base.py:678\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    674\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[1;32m    676\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 678\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    679\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    682\u001b[0m has_sw \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    619\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1142\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1144\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[0;32m-> 1147\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1148\u001b[0m     X,\n\u001b[1;32m   1149\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1150\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1151\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1152\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1153\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1154\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1155\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1156\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1157\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1158\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1159\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1160\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1161\u001b[0m )\n\u001b[1;32m   1163\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/sklearn/utils/validation.py:978\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    976\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    977\u001b[0m     \u001b[39mif\u001b[39;00m n_features \u001b[39m<\u001b[39m ensure_min_features:\n\u001b[0;32m--> 978\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    979\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m feature(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    980\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m a minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m             \u001b[39m%\u001b[39m (n_features, array\u001b[39m.\u001b[39mshape, ensure_min_features, context)\n\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[1;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m    986\u001b[0m         \u001b[39m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(2, 0)) while a minimum of 1 is required by LinearRegression."
     ]
    }
   ],
   "source": [
    "# Import the class containing the regression method\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create an instance of the class\n",
    "LR = LinearRegression()\n",
    "\n",
    "# Fit the instance on the data and then predict the expected value\n",
    "LR = LR.fit(X_train, y_train)\n",
    "y_predict = LR.predict(X_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

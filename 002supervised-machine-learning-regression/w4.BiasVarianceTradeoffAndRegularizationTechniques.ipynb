{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade off and Regularization Techniques: Ridge, LASSO, and Elastic Net\n",
    "\n",
    "## Bias Variance Trade off\n",
    "\n",
    "Learning Goals\n",
    "- Model complexity vs. error\n",
    "- Bias and variance of a model\n",
    "- Sources of model error\n",
    "- The bias-variance tradeoff\n",
    "\n",
    "3 Sources of Model Error\n",
    "- Bias: being wrong\n",
    "  - Tendency of predictions to miss true values.\n",
    "    - Worsened by missing information, overly-simplistic assumptions.\n",
    "    - Miss real patterns (underfit).\n",
    "- Variance: being unstable\n",
    "  - Tendency of predictions to fluctuate.\n",
    "    - Characterized by sensitivity or output to small changes in input data.\n",
    "    - Often due to overly complex or poorly-fit models.\n",
    "- Irreducible Error: unavoidable randomness\n",
    "  - Tendency to intrinsic uncertainty/randomness.\n",
    "    - Present in even the best possible model.\n",
    "\n",
    "\n",
    "Summary of bias-variance tradeoff:\n",
    "- Model adjustments that decrease bias\n",
    "often increase variance, and vice versa.\n",
    "- The bias-variance tradeoff is analogous\n",
    "to a complexity tradeoff.\n",
    "- Finding the best model means choosing\n",
    "the right level of complexity.\n",
    "- Want a model elaborate enough to not\n",
    "underfit, but not so exceedingly\n",
    "elaborate that it overfits.\n",
    "\n",
    "Discussion:\n",
    "- The higher the degree of a polynomial regression,\n",
    "the more complex the model (lower bias, higher variance).\n",
    "- At lower degrees, we can see visual signs of bias:\n",
    "predictions are too rigid to capture the curve pattern in the data.\n",
    "- At higher degrees, we see visual signs of variance:\n",
    "predictions fluctuate wildly because of the model's sensitivity.\n",
    "- The goal is to find the right degree, such that the model\n",
    "has sufficient complexity to describe the data without overfitting.\n",
    "\n",
    "## Regularization and Model Selection\n",
    "\n",
    "Topics:\n",
    "\n",
    "- Model complexity and error\n",
    "\n",
    "- Regularization as an approach to over-fitting\n",
    "\n",
    "- Standard approaches to regularization including Ridge, Lasso, and Elastic Net\n",
    "\n",
    "- Recursive feature elimination\n",
    "\n",
    "### Tuning the Model\n",
    "\n",
    "Can we tune with more granularity than choosing polynomial degrees?\n",
    "\n",
    "Yes, by using regularization.\n",
    "\n",
    "#### What does Regularization Accomplish?\n",
    "\n",
    "\n",
    "Adjusted cost function: $ M(w) + \\lambda R(w) $\n",
    "\n",
    "M(w) : model error\n",
    "\n",
    "R(w): function of estimated parameter(s)\n",
    "\n",
    "$\\lambda$ : regularization strength parameter\n",
    "\n",
    "Regularization adds an (adjustable)\n",
    "regularization strength parameter directly\n",
    "into the cost function.\n",
    "\n",
    "This $\\lambda$ (lambda) adds a penalty proportional\n",
    "to the size of the estimated model\n",
    "parameter, or a function of the parameter.\n",
    "\n",
    "Increasing the cost function controls the\n",
    "amount of the penalty.\n",
    "\n",
    "But the takeaway is that the larger this lambda is, the more we penalize stronger parameters. And again, the more we penalize our model for being stronger and having stronger parameters, the less complex that model will be able to be as we try to minimize this function, right? That'll make it so that we are trying to minimize the strength of all of our parameters while minimizing our original cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization strength parameter $\\lambda$\n",
    "(lambda) allows us to manage\n",
    "the\n",
    "complexity tradeoff:\n",
    "- more regularization introduces\n",
    "a simpler model or more bias.\n",
    "- less regularization makes the model\n",
    "more complex and increases variance.\n",
    "\n",
    "If our model is overfit (variance too high),\n",
    "regularization can improve the\n",
    "generalization error and reduce variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization and Feature Selection\n",
    "\n",
    "Regularization performs feature selection by shrinking the contribution of features.\n",
    "\n",
    "For L1-regularization, this is accomplished by driving some coefficients to zero.\n",
    "\n",
    "Feature selection can also be performed by removing features.\n",
    "\n",
    "### Why is Feature Selection Important?\n",
    "\n",
    "Reducing the number of features can prevent overfitting.\n",
    "\n",
    "For some models, fewer features can improve fitting time and/or results.\n",
    "\n",
    "Identifying most critical features can improve model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "![Reg Cost Function: Ridge Regression](./images/08_RegCostFunctionRidgeRegression.png \"Reg Cost Function: Ridge Regression\")\n",
    "\n",
    "### Standard scaling \n",
    "\n",
    "![Scale matter](./images/09_Scale_matter.png \"Scale matter\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression:\n",
    "\n",
    "the complexity penalty is applied proportionally to squared coefficient values.\n",
    "\n",
    "- The penalty term has the effect of \"shrinking\" coefficients toward 0.\n",
    "- This imposes bias on the model, but also reduces variance.\n",
    "- We can select the best regularization strength lambda via cross-validation.\n",
    "- It's best practice to scale features (i.e. using StandardScaler)\n",
    "so penalties aren't impacted by variable scale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\beta_0,\\beta_1)=\\dfrac{1}{2m}\\sum_{i=1}^m((\\beta_0+\\beta_1 x_{obs}^i)-y_{obs}^i)^2 + \\lambda\\sum_{j=1}^k\\beta_j^2$$\n",
    "\n",
    "Penalty shrinks magnitude of all coefficients\n",
    "\n",
    "Larger coefficients strongly penalized because of the squaring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ridge Regression in Actior](./images/10_RidgeRegressionInAction.png \"Ridge Regression in Actior\")\n",
    "\n",
    "![Complexity Tradeoff](./images/11_ComplexityTradeoff.png \"Complexity Tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "![Lasso Regression](./images/12_LassoRegression.png \"Lasso Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge or L_2, we use the coefficient squared and with LASSO we'll be using the absolute value of each one of these coefficients.\n",
    "\n",
    "In LASSO regression: the complexity penalty ^ (lambda) is proportional to\n",
    "the absolute value of coefficients.\n",
    "- LASSO: Least Absolute Shrinkage and Selection Operator.\n",
    "- Similar effect to Ridge in terms of complexity tradeoff:\n",
    "increasing lambda raises bias but lowers variance.\n",
    "- LASSO is more likely than Ridge to perform feature selection,\n",
    "in that for a fixed A, LASSO is more likely to result in coefficients being set to zero.\n",
    "\n",
    "$$ J(\\beta_0,\\beta_1)=\\dfrac{1}{2m}\\sum_{i=1}^m((\\beta_0+\\beta_1 x_{obs}^i)-y_{obs}^i)^2 + \\lambda\\sum_{j=1}^k|\\beta_j|$$\n",
    "\n",
    "Penalty selectively shrinks some coefficients.\n",
    "\n",
    "Can be used for feature selection.\n",
    "\n",
    "Slower to converge than Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between Ridge and Lasso : Elastic Net \n",
    "\n",
    "![Elastic Net ](./images/13_ElasticNet.png \"Elastic Net \")\n",
    "\n",
    "$$ J(\\beta_0,\\beta_1)=\\dfrac{1}{2m}\\sum_{i=1}^m((\\beta_0+\\beta_1 x_{obs}^i)-y_{obs}^i)^2 + \\lambda\\sum_{j=1}^k|\\beta_j| + \\lambda_2\\sum_{j=1}^k\\beta_j^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net combines penalties from both Ridge and LASSO regression.\n",
    "\n",
    "It requires tuning of an additional parameter that determines emphasis\n",
    "of L1 VS. L2 regularization penalties.\n",
    "\n",
    "The differences between L1 and L2 regularization:\n",
    "\n",
    "L1 regularization penalizes the sum of absolute values of the weights, whereas L2 regularization penalizes the sum of squares of the weights. The L1 regularization solution is sparse. The L2 regularization solution is non-sparse.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination (RFE) is an approach that combines:\n",
    "- A model or estimation approach\n",
    "- A desired number of features\n",
    "RFE then repeatedly applies the model, measures feature importance,\n",
    "and recursively removes less important features.\n",
    "\n",
    "### Recursive Feature Elimination: The Syntax\n",
    "Import the class containing the feature selection method\n",
    "```python \n",
    "from sklearn.feature_selection import RFE\n",
    "```\n",
    "Create an instance of the class\n",
    "```python\n",
    "rfeMod = RFE(est, n_features_to_select=5)\n",
    "```\n",
    "Fit the instance on the data and then predict the expected value\n",
    "```python\n",
    "rfeMod = rfeMod.fit(X_train, y_train)\n",
    "y_predict = rfeMod.predict(X_test)\n",
    "```\n",
    "The RFECV class will perform feature elimination using cross validation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"run_control":{"marked":true}},"source":["# Machine Learning Foundation\n","\n","## Course 5, Part g: Transfer Learning DEMO\n"]},{"cell_type":"markdown","metadata":{},"source":["For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n","\n","Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import datetime\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","#from tensorflow import keras\n","#from tensorflow.keras.datasets import mnist\n","#from tensorflow.keras.models import Sequential\n","#from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","#from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","#from tensorflow.keras import backend as K"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#used to help some of the timing functions\n","now = datetime.datetime.now"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# set some parameters\n","batch_size = 128\n","num_classes = 5\n","epochs = 5"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# set some more parameters\n","img_rows, img_cols = 28, 28\n","filters = 32\n","pool_size = 2\n","kernel_size = 3"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["## This just handles some variability in how the input data is loaded\n","\n","if K.image_data_format() == 'channels_first':\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    input_shape = (img_rows, img_cols, 1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["## To simplify things, write a function to include all the training steps\n","## As input, function takes a model, training set, test set, and the number of classes\n","## Inside the model object will be the state about which layers we are freezing and which we are training\n","\n","def train_model(model, train, test, num_classes):\n","    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n","    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n","    x_train = x_train.astype('float32')\n","    x_test = x_test.astype('float32')\n","    x_train /= 255\n","    x_test /= 255\n","    print('x_train shape:', x_train.shape)\n","    print(x_train.shape[0], 'train samples')\n","    print(x_test.shape[0], 'test samples')\n","\n","    # convert class vectors to binary class matrices\n","    y_train = keras.utils.to_categorical(train[1], num_classes)\n","    y_test = keras.utils.to_categorical(test[1], num_classes)\n","\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer='adadelta',\n","                  metrics=['accuracy'])\n","\n","    t = now()\n","    model.fit(x_train, y_train,\n","              batch_size=batch_size,\n","              epochs=epochs,\n","              verbose=1,\n","              validation_data=(x_test, y_test))\n","    print('Training time: %s' % (now() - t))\n","\n","    score = model.evaluate(x_test, y_test, verbose=0)\n","    print('Test score:', score[0])\n","    print('Test accuracy:', score[1])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# the data, shuffled and split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# create two datasets: one with digits below 5 and one with 5 and above\n","x_train_lt5 = x_train[y_train < 5]\n","y_train_lt5 = y_train[y_train < 5]\n","x_test_lt5 = x_test[y_test < 5]\n","y_test_lt5 = y_test[y_test < 5]\n","\n","x_train_gte5 = x_train[y_train >= 5]\n","y_train_gte5 = y_train[y_train >= 5] - 5\n","x_test_gte5 = x_test[y_test >= 5]\n","y_test_gte5 = y_test[y_test >= 5] - 5"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n","# to a new problem.  We will freeze these layers during the fine-tuning process\n","\n","feature_layers = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n","# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n","\n","classification_layers = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# We create our model by combining the two sets of layers as follows\n","model = Sequential(feature_layers + classification_layers)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," activation (Activation)     (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_1 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               589952    \n","                                                                 \n"," activation_2 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_3 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 600165 (2.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Let's take a look\n","model.summary()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","Epoch 1/5\n","230/230 [==============================] - 6s 24ms/step - loss: 1.6223 - accuracy: 0.2025 - val_loss: 1.5922 - val_accuracy: 0.2592\n","Epoch 2/5\n","230/230 [==============================] - 5s 24ms/step - loss: 1.5850 - accuracy: 0.2637 - val_loss: 1.5491 - val_accuracy: 0.4707\n","Epoch 3/5\n","230/230 [==============================] - 6s 24ms/step - loss: 1.5438 - accuracy: 0.3457 - val_loss: 1.5028 - val_accuracy: 0.6449\n","Epoch 4/5\n","230/230 [==============================] - 6s 25ms/step - loss: 1.5025 - accuracy: 0.4222 - val_loss: 1.4525 - val_accuracy: 0.7108\n","Epoch 5/5\n","230/230 [==============================] - 5s 24ms/step - loss: 1.4561 - accuracy: 0.4877 - val_loss: 1.3963 - val_accuracy: 0.7474\n","Training time: 0:00:28.109342\n","Test score: 1.396318793296814\n","Test accuracy: 0.7473770976066589\n"]}],"source":["# Now, let's train our model on the digits 5,6,7,8,9\n","\n","train_model(model,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["### Freezing Layers\n","Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n","\n","Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set.\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Freeze only the feature layers\n","for l in feature_layers:\n","    l.trainable = False"]},{"cell_type":"markdown","metadata":{},"source":["Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*.\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 26, 26, 32)        320       \n","                                                                 \n"," activation (Activation)     (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_1 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 12, 12, 32)        0         \n"," D)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten (Flatten)           (None, 4608)              0         \n","                                                                 \n"," dense (Dense)               (None, 128)               589952    \n","                                                                 \n"," activation_2 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_3 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 590597 (2.25 MB)\n","Non-trainable params: 9568 (37.38 KB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 3s 12ms/step - loss: 1.5734 - accuracy: 0.3081 - val_loss: 1.5164 - val_accuracy: 0.5279\n","Epoch 2/5\n","240/240 [==============================] - 3s 11ms/step - loss: 1.5052 - accuracy: 0.4070 - val_loss: 1.4468 - val_accuracy: 0.6558\n","Epoch 3/5\n","240/240 [==============================] - 3s 11ms/step - loss: 1.4425 - accuracy: 0.4987 - val_loss: 1.3791 - val_accuracy: 0.7270\n","Epoch 4/5\n","240/240 [==============================] - 3s 11ms/step - loss: 1.3818 - accuracy: 0.5725 - val_loss: 1.3133 - val_accuracy: 0.7893\n","Epoch 5/5\n","240/240 [==============================] - 3s 11ms/step - loss: 1.3236 - accuracy: 0.6338 - val_loss: 1.2499 - val_accuracy: 0.8453\n","Training time: 0:00:13.860146\n","Test score: 1.2498853206634521\n","Test accuracy: 0.8453006148338318\n"]}],"source":["train_model(model,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"markdown","metadata":{},"source":["Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n","\n","Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. \n"]},{"cell_type":"markdown","metadata":{},"source":["## Exercise\n","- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9.\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_4 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_5 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_6 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_7 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 600165 (2.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Create layers and define the model as above\n","feature_layers2 = [\n","    Conv2D(filters, kernel_size,\n","           padding='valid',\n","           input_shape=input_shape),\n","    Activation('relu'),\n","    Conv2D(filters, kernel_size),\n","    Activation('relu'),\n","    MaxPooling2D(pool_size=pool_size),\n","    Dropout(0.25),\n","    Flatten(),\n","]\n","\n","classification_layers2 = [\n","    Dense(128),\n","    Activation('relu'),\n","    Dropout(0.5),\n","    Dense(num_classes),\n","    Activation('softmax')\n","]\n","model2 = Sequential(feature_layers2 + classification_layers2)\n","model2.summary()"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (30596, 28, 28, 1)\n","30596 train samples\n","5139 test samples\n","Epoch 1/5\n","240/240 [==============================] - 6s 25ms/step - loss: 1.6197 - accuracy: 0.1825 - val_loss: 1.5921 - val_accuracy: 0.3318\n","Epoch 2/5\n","240/240 [==============================] - 6s 24ms/step - loss: 1.5774 - accuracy: 0.2818 - val_loss: 1.5462 - val_accuracy: 0.4384\n","Epoch 3/5\n","240/240 [==============================] - 6s 27ms/step - loss: 1.5343 - accuracy: 0.3703 - val_loss: 1.4975 - val_accuracy: 0.5264\n","Epoch 4/5\n","240/240 [==============================] - 6s 25ms/step - loss: 1.4885 - accuracy: 0.4397 - val_loss: 1.4430 - val_accuracy: 0.6165\n","Epoch 5/5\n","240/240 [==============================] - 6s 25ms/step - loss: 1.4354 - accuracy: 0.5063 - val_loss: 1.3801 - val_accuracy: 0.7285\n","Training time: 0:00:30.565686\n","Test score: 1.3800685405731201\n","Test accuracy: 0.7285463809967041\n"]}],"source":["# Now, let's train our model on the digits 0,1,2,3,4\n","train_model(model2,\n","            (x_train_lt5, y_train_lt5),\n","            (x_test_lt5, y_test_lt5), num_classes)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#Freeze layers\n","for l in feature_layers2:\n","    l.trainable = False"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n","                                                                 \n"," activation_4 (Activation)   (None, 26, 26, 32)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n","                                                                 \n"," activation_5 (Activation)   (None, 24, 24, 32)        0         \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n"," g2D)                                                            \n","                                                                 \n"," dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 4608)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               589952    \n","                                                                 \n"," activation_6 (Activation)   (None, 128)               0         \n","                                                                 \n"," dropout_3 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 645       \n","                                                                 \n"," activation_7 (Activation)   (None, 5)                 0         \n","                                                                 \n","=================================================================\n","Total params: 600165 (2.29 MB)\n","Trainable params: 590597 (2.25 MB)\n","Non-trainable params: 9568 (37.38 KB)\n","_________________________________________________________________\n"]}],"source":["model2.summary()"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x_train shape: (29404, 28, 28, 1)\n","29404 train samples\n","4861 test samples\n","Epoch 1/5\n","230/230 [==============================] - 3s 12ms/step - loss: 1.5869 - accuracy: 0.3120 - val_loss: 1.5539 - val_accuracy: 0.4407\n","Epoch 2/5\n","230/230 [==============================] - 3s 11ms/step - loss: 1.5534 - accuracy: 0.3512 - val_loss: 1.5177 - val_accuracy: 0.4906\n","Epoch 3/5\n","230/230 [==============================] - 3s 12ms/step - loss: 1.5210 - accuracy: 0.4036 - val_loss: 1.4823 - val_accuracy: 0.5517\n","Epoch 4/5\n","230/230 [==============================] - 3s 12ms/step - loss: 1.4897 - accuracy: 0.4500 - val_loss: 1.4476 - val_accuracy: 0.6202\n","Epoch 5/5\n","230/230 [==============================] - 3s 12ms/step - loss: 1.4563 - accuracy: 0.5022 - val_loss: 1.4133 - val_accuracy: 0.6873\n","Training time: 0:00:13.757512\n","Test score: 1.4133015871047974\n","Test accuracy: 0.6873071193695068\n"]}],"source":["train_model(model2,\n","            (x_train_gte5, y_train_gte5),\n","            (x_test_gte5, y_test_gte5), num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save('data/05f_DemoTransferLearning.keras')"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}

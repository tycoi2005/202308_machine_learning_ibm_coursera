{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network And Long Short Team Memory Networks\n",
    "\n",
    "In this module you become familiar with Recursive Neural Networks (RNNs) and Long-Short Term Memory Networks (LSTM), a type of RNN considered the breakthrough for speech to text recongintion. RNNs are frequently used in most AI applications today, and can also be used for supervised learning. \n",
    "\n",
    "Learning Objectives\n",
    "- Explain how a Recurrent Neural Network works\n",
    "- Become familiar with the most common architectures for Recurrent Neural Networks\n",
    "- Gain practice using RNNs and LSTM for classification and image applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In this section, we will cover:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Practical and Mathematical details\n",
    "- Limitations of RNNs in practice\n",
    "\n",
    "### Variable Length Sequences of Words\n",
    "\n",
    "Processing of images often forces them into a specific input dimension.\n",
    "- Not obvious how to do this with text.\n",
    "- For example: classify tweets as positive, negative, or neutral.\n",
    "  - Tweets can have a variable number of words.\n",
    "  - What to do?\n",
    "\n",
    "### Ordering of Words is Important\n",
    "\n",
    "Want to do better than \"bag of words\" implementations.\n",
    "- Ideally, each word is processed or understood in the appropriate context\n",
    "(but need to have some notion of \"context\").\n",
    "- Words should be handled differently depending on \"context\".\n",
    "- Also, each word should update the context.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Input words one by one.\n",
    "- This way, we can handle variable lengths of text.\n",
    "- The response to a word depends on the words that preceded it.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Network outputs two things:\n",
    "- Prediction: What would be the prediction if the sequence ended with that word.\n",
    "- State: Summary of everything that happened in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Recurrent Neural Networks\n",
    "\n",
    "![](./images/46_RecurrenceNet.png)\n",
    "\n",
    "![](./images/47_UnrollingTheRNN.png)\n",
    "\n",
    "- $w_i$ : i_th word\n",
    "- U: Matrix U - linear transformation \n",
    "- W: vector cointain informations \n",
    "- $s_i$ : state at i, $s_0$ with input word is vector zero\n",
    "- V : Matrix V - another  transformation\n",
    "- $o_i$ : output\n",
    "\n",
    "From bottom to top: Kernel parts - recurrent parts - Dense parts\n",
    "\n",
    "![](./images/48_UnrollingTheRNN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory (LSTM) Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit Details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

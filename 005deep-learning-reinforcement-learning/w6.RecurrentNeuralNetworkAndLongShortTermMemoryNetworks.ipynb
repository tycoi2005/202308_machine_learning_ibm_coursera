{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network And Long Short Team Memory Networks\n",
    "\n",
    "In this module you become familiar with Recursive Neural Networks (RNNs) and Long-Short Term Memory Networks (LSTM), a type of RNN considered the breakthrough for speech to text recongintion. RNNs are frequently used in most AI applications today, and can also be used for supervised learning. \n",
    "\n",
    "Learning Objectives\n",
    "- Explain how a Recurrent Neural Network works\n",
    "- Become familiar with the most common architectures for Recurrent Neural Networks\n",
    "- Gain practice using RNNs and LSTM for classification and image applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In this section, we will cover:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Practical and Mathematical details\n",
    "- Limitations of RNNs in practice\n",
    "\n",
    "### Variable Length Sequences of Words\n",
    "\n",
    "Processing of images often forces them into a specific input dimension.\n",
    "- Not obvious how to do this with text.\n",
    "- For example: classify tweets as positive, negative, or neutral.\n",
    "  - Tweets can have a variable number of words.\n",
    "  - What to do?\n",
    "\n",
    "### Ordering of Words is Important\n",
    "\n",
    "Want to do better than \"bag of words\" implementations.\n",
    "- Ideally, each word is processed or understood in the appropriate context\n",
    "(but need to have some notion of \"context\").\n",
    "- Words should be handled differently depending on \"context\".\n",
    "- Also, each word should update the context.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Input words one by one.\n",
    "- This way, we can handle variable lengths of text.\n",
    "- The response to a word depends on the words that preceded it.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Network outputs two things:\n",
    "- Prediction: What would be the prediction if the sequence ended with that word.\n",
    "- State: Summary of everything that happened in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Recurrent Neural Networks\n",
    "\n",
    "![](./images/46_RecurrenceNet.png)\n",
    "\n",
    "![](./images/47_UnrollingTheRNN.png)\n",
    "\n",
    "- $w_i$ : i_th word\n",
    "- U: Matrix U - linear transformation \n",
    "- W: vector cointain informations \n",
    "- $s_i$ : state at i, $s_0$ with input word is vector zero\n",
    "- V : Matrix V - another  transformation\n",
    "- $o_i$ : output\n",
    "\n",
    "From bottom to top: Kernel parts - recurrent parts - Dense parts\n",
    "\n",
    "![](./images/48_UnrollingTheRNN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details Recurrent Neural Networks\n",
    "\n",
    "### Mathematical Details\n",
    "\n",
    "$W_i$ is the word at position i\n",
    "\n",
    "$s_i$ is the state at position i\n",
    "\n",
    "$o_i$ is the output at position\n",
    "\n",
    "$S_i = f(Uw_i + Ws_{i-1} $ \n",
    "(Core RNN)\n",
    "\n",
    "$o_i = softmax (Vs_i)$\n",
    "(subsequent dense layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "- current state = function1 (old state, current input).\n",
    "- current output = function2(current state).\n",
    "- We learn function1 and function2 by training our network!\n",
    "\n",
    "r= dimension of input vector\n",
    "\n",
    "S = dimension of hidden state\n",
    "\n",
    "t = dimension of output vector (after dense layer)\n",
    "\n",
    "U is a s x r matrix\n",
    "\n",
    "W is as Ã— s matrix\n",
    "\n",
    "V is at x s matrix\n",
    "\n",
    "Note: The weight matrices U, V, W are the same across all positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Details\n",
    "\n",
    "Often, we train on just the \"final\" output and ignore intermediate outputs.\n",
    "- Slight variation called Backpropagation Through Time (BPTT) is used to train RNNs.\n",
    "- Sensitive to length of sequence (due to \"vanishing/exploding gradient\" problem).\n",
    "\n",
    "In practice, we still set a maximum length to our sequences.\n",
    "- If input is shorter than maximum, we \"pad\" it.\n",
    "- If input is longer than maximum, we truncate.\n",
    "\n",
    "### Other Uses of RNNs\n",
    "\n",
    "RNNs often focus on text applications.\n",
    "\n",
    "But, RNNs can be used for other sequential data:\n",
    "- Forecasting: Customer Sales, Loss Rates, Network Traffic.\n",
    "- Speech Recognition: Call Center Automation, Voice Applications.\n",
    "- Manufacturing Sensor Data\n",
    "- Genome Sequences\n",
    "\n",
    "### Weaknesses of RNNs\n",
    "\n",
    "Nature of state transition means is hard to keep information\n",
    "from distant past in current memory without reinforcement.\n",
    "\n",
    "In the next lecture, we introduce LSTMs,\n",
    "these have a more complex mechanism for updating the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory (LSTM) Networks\n",
    "\n",
    "In this section, we will cover:\n",
    "- Long Short-term Memory Networks (LSTMs)\n",
    "- Gated Recurrent Units (GRUs)\n",
    "- Sequence-to-Sequence Models (Seq2Seq)\n",
    "- Common enterprise applications of LSTM models\n",
    "\n",
    "### Standard RNNs Have Poor Memory\n",
    "\n",
    "Transition Matrix necessarily weakens signal.\n",
    "- Need a structure that can leave some dimensions unchanged over many steps.\n",
    "- This is the problem addressed by so-called Long-Short Term Memory RNNs (LSTM).\n",
    "\n",
    "### Make \"Remembering\" Easy\n",
    "\n",
    "Define a more complicated update mechanism for the changing of the internal state.\n",
    "- By default, LSTMs remember the information from the last step.\n",
    "- Items are overwritten as an active choice.\n",
    "\n",
    "### Long Short-term Memory Networks\n",
    "\n",
    "LSTMs are a special kind of RNN (invented in 1997).\n",
    "- \"State of the art\" (the idea is old, but the available computing power is new)\n",
    "for many sequence to sequence mapping and text generation tasks.\n",
    "- Adds an explicit \"memory\" unit.\n",
    "\n",
    "Augment RNNs with a few additional Gate Units:\n",
    "- Gate Units control how long/if events will stay in memory.\n",
    "- Input Gate: If its value is such, it causes items to be stored in memory.\n",
    "- Forget Gate: If its value is such, it causes items to be removed from memory.\n",
    "- Output Gate: If its value is such, it causes the hidden unit to feed forward (output)\n",
    "in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Explanation\n",
    "\n",
    "![](./images/49_LSTMDiagram.png)\n",
    "\n",
    "![](./images/50_LSTMDiagram.png)\n",
    "\n",
    "![](./images/51_LSTMForget.png)\n",
    "\n",
    "![](./images/52_LSTMAddNewInfo.png)\n",
    "\n",
    "![](./images/53_LSTMRemembering.png)\n",
    "\n",
    "![](./images/54_LSTMOutput.png)\n",
    "\n",
    "![](./images/55_LSTMUnrolled.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit\n",
    "\n",
    "Removed Cell State:\n",
    "- used Past information is now\n",
    "to transfer past information.\n",
    "- Think of as a \"simpler\"\n",
    "and faster version of LSTM.\n",
    "\n",
    "![](./images/56_GatedRecurrentUnit.png)\n",
    "\n",
    "![](./images/57_ResetUpdateGate.png)\n",
    "\n",
    "### LSTM or GRU?\n",
    "\n",
    "LSTMs are a bit more complex and may therefore be able to find more complicated patterns\n",
    "- Conversely, GRUs train. are a bit simpler and therefore are quicker to\n",
    "- GRUs will generally perform  about as  well as  LSTMs with shorter training time,\n",
    "especially for smaller datasets.\n",
    "- Luckily in Keras all we need to do is call the layer type and it would not be too complicated to quickly write up changes\n",
    "between the two. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./images/58_Seq2Seq.png)\n",
    "\n",
    "![](./images/59_Seq2Seq.png)\n",
    "\n",
    "![](./images/60_Seq2SeqMachineTranslation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit Details\n",
    "\n",
    "### Greedy Inference\n",
    "Note:\n",
    "- Currently model is producing one word at a\n",
    "time conditional on prior word. \n",
    "- If it produces one wrong word,\n",
    " we may end up completely\n",
    "throwing off the sequence of words. \n",
    "\n",
    "![](./images/61_BeamSeach.png)\n",
    "\n",
    "Current framework: the generative process is using all information in the final hidden layer. \n",
    "- i.e. each decoder time step depends on the same encoder embedding.\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

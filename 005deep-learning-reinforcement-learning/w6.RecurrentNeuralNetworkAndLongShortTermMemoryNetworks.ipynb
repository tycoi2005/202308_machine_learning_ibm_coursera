{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network And Long Short Team Memory Networks\n",
    "\n",
    "In this module you become familiar with Recursive Neural Networks (RNNs) and Long-Short Term Memory Networks (LSTM), a type of RNN considered the breakthrough for speech to text recongintion. RNNs are frequently used in most AI applications today, and can also be used for supervised learning. \n",
    "\n",
    "Learning Objectives\n",
    "- Explain how a Recurrent Neural Network works\n",
    "- Become familiar with the most common architectures for Recurrent Neural Networks\n",
    "- Gain practice using RNNs and LSTM for classification and image applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In this section, we will cover:\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "- Practical and Mathematical details\n",
    "- Limitations of RNNs in practice\n",
    "\n",
    "### Variable Length Sequences of Words\n",
    "\n",
    "Processing of images often forces them into a specific input dimension.\n",
    "- Not obvious how to do this with text.\n",
    "- For example: classify tweets as positive, negative, or neutral.\n",
    "  - Tweets can have a variable number of words.\n",
    "  - What to do?\n",
    "\n",
    "### Ordering of Words is Important\n",
    "\n",
    "Want to do better than \"bag of words\" implementations.\n",
    "- Ideally, each word is processed or understood in the appropriate context\n",
    "(but need to have some notion of \"context\").\n",
    "- Words should be handled differently depending on \"context\".\n",
    "- Also, each word should update the context.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Input words one by one.\n",
    "- This way, we can handle variable lengths of text.\n",
    "- The response to a word depends on the words that preceded it.\n",
    "\n",
    "### Idea: Use the Notion of \"Recurrence\"\n",
    "\n",
    "Network outputs two things:\n",
    "- Prediction: What would be the prediction if the sequence ended with that word.\n",
    "- State: Summary of everything that happened in the past."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Recurrent Neural Networks\n",
    "\n",
    "![](./images/46_RecurrenceNet.png)\n",
    "\n",
    "![](./images/47_UnrollingTheRNN.png)\n",
    "\n",
    "- $w_i$ : i_th word\n",
    "- U: Matrix U - linear transformation \n",
    "- W: vector cointain informations \n",
    "- $s_i$ : state at i, $s_0$ with input word is vector zero\n",
    "- V : Matrix V - another  transformation\n",
    "- $o_i$ : output\n",
    "\n",
    "From bottom to top: Kernel parts - recurrent parts - Dense parts\n",
    "\n",
    "![](./images/48_UnrollingTheRNN.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details Recurrent Neural Networks\n",
    "\n",
    "### Mathematical Details\n",
    "\n",
    "$W_i$ is the word at position i\n",
    "\n",
    "$s_i$ is the state at position i\n",
    "\n",
    "$o_i$ is the output at position\n",
    "\n",
    "$S_i = f(Uw_i + Ws_{i-1} $ \n",
    "(Core RNN)\n",
    "\n",
    "$o_i = softmax (Vs_i)$\n",
    "(subsequent dense layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words:\n",
    "- current state = function1 (old state, current input).\n",
    "- current output = function2(current state).\n",
    "- We learn function1 and function2 by training our network!\n",
    "\n",
    "r= dimension of input vector\n",
    "\n",
    "S = dimension of hidden state\n",
    "\n",
    "t = dimension of output vector (after dense layer)\n",
    "\n",
    "U is a s x r matrix\n",
    "\n",
    "W is as Ã— s matrix\n",
    "\n",
    "V is at x s matrix\n",
    "\n",
    "Note: The weight matrices U, V, W are the same across all positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Details\n",
    "\n",
    "Often, we train on just the \"final\" output and ignore intermediate outputs.\n",
    "- Slight variation called Backpropagation Through Time (BPTT) is used to train RNNs.\n",
    "- Sensitive to length of sequence (due to \"vanishing/exploding gradient\" problem).\n",
    "\n",
    "In practice, we still set a maximum length to our sequences.\n",
    "- If input is shorter than maximum, we \"pad\" it.\n",
    "- If input is longer than maximum, we truncate.\n",
    "\n",
    "### Other Uses of RNNs\n",
    "\n",
    "RNNs often focus on text applications.\n",
    "\n",
    "But, RNNs can be used for other sequential data:\n",
    "- Forecasting: Customer Sales, Loss Rates, Network Traffic.\n",
    "- Speech Recognition: Call Center Automation, Voice Applications.\n",
    "- Manufacturing Sensor Data\n",
    "- Genome Sequences\n",
    "\n",
    "### Weaknesses of RNNs\n",
    "\n",
    "Nature of state transition means is hard to keep information\n",
    "from distant past in current memory without reinforcement.\n",
    "\n",
    "In the next lecture, we introduce LSTMs,\n",
    "these have a more complex mechanism for updating the state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-Short Term Memory (LSTM) Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gated Recurrent Unit Details"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

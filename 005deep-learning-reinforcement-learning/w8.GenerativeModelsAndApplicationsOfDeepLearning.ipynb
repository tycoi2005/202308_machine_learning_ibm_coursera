{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative models and Application of Deep Learning\n",
    "\n",
    "In this module, you will learn about two types of generative models, which are Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We will look at the theory behind each model and then implement them in Keras for generating artificial images. The goal is usually to generate images that are as realistic as possible. In the last lesson of this module, we will touch on additional topics in deep learning, namely using Keras in a GPU environment for speeding up model training.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Understand the applications of GANs\n",
    "\n",
    "Understand the differences between Autoencoders and VAEs\n",
    "\n",
    "Build and train generative models in Keras for different applications\n",
    "\n",
    "Describe the role of Generator and Discriminator in GANs\n",
    "\n",
    "Describe the working mechanics of GANs\n",
    "\n",
    "Gain understanding on best practices for developing VAEs\n",
    "\n",
    "Gain practice of implementing VAEs in Keras\n",
    "\n",
    "Explain the theory behind Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Review\n",
    "\n",
    "## Variational Autoencoders\n",
    "Variational autoencoders also generate a latent representation and then use this representation to generate new samples (i.e. images). \n",
    "\n",
    "These are some important features of variational autoencoders:\n",
    "\n",
    "- Data are assumed to be represented by a set of normally-distributed latent factors.\n",
    "\n",
    "- The encoder generates parameters of these distributions, namely µ and σ.\n",
    "\n",
    "- Images can be generated by sampling from these distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE goals\n",
    "\n",
    "The main goal of VAEs: generate images using the decoder\n",
    "\n",
    "The secondary goal is to have similar images be close together in latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function of Variational Autoencoders\n",
    "\n",
    "The VAE reconstruct the original images from the space of vectors drawn from a standard normal distribution.\n",
    "\n",
    "The two components of the loss function are:\n",
    "\n",
    "- A penalty for not reconstructing the image correctly. \n",
    "\n",
    "- A penalty for generating vectors of parameters µ and σ that are different than 0 and 1, respectively: the parameters of the standard normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "\n",
    "The invention of GANs was connected to neural networks’ vulnerability to adversarial examples. Researchers were going to run a speech synthesis contest, to see which neural network could generate the most realistic-sounding speech.\n",
    "\n",
    "A neural network - the “discriminator” - would judge whether the speech was real or not.\n",
    "\n",
    "In the end, they decided not to run the contest, because they realized people would generate speech to fool this particular network, rather than actually generating realistic speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the step to train GANs:\n",
    "\n",
    "Randomly initialize weights of generator and discriminator networks\n",
    "\n",
    "Randomly initialize noise vector and generate image using generator\n",
    "\n",
    "Predict probability generated image is real using discriminator\n",
    "\n",
    "Compute losses both assuming the image was fake and assuming it was real\n",
    "\n",
    "Train the discriminator to output whether the image is fake\n",
    "\n",
    "Compute the penalty for the discriminator probability, without using it to train the discriminator\n",
    "\n",
    "Train the generator to generate images that the discriminator thinks are real\n",
    "\n",
    "Use the discriminator to calculate the probability that a real image is real\n",
    "\n",
    "Use L to train the discriminator to output 1 when it sees real images"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "In this module you become familiar with other novel applications of Neural Networks. You will learn about Generative Adversarial Networks, frequently referred to as GANs, which are an application of Neural Networks to generate new data. Finally, you learn about Reinforcement Learning, one of the big promises for A.I., based on training algorithms by using rewards, instead of using a method to minimize error, which is what we have been using throughout the course.\n",
    "\n",
    "Learning Objectives\n",
    "- Explain the theory behind Reinforcement Learning\n",
    "- Become familiar with Reinforcement Learning and its applications\n",
    "- Gain practice using Keras and sklearn for Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary/Review\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "In Reinforcement Learning, Agents interact with an Environment\n",
    "\n",
    "They choose from a set of available Actions\n",
    "\n",
    "The actions impact the Environment, which impacts agents via Rewards\n",
    "\n",
    "Rewards are generally unknown and must be estimated by the agent\n",
    "\n",
    "The process repeats dynamically, so agents learn how to estimate rewards over time\n",
    "\n",
    "Advances in deep learning have led to many recent RL developments:\n",
    "\n",
    "- In 2013, researchers from DeepMind developed a system to play Atari games\n",
    "\n",
    "- In 2017, the AlphaGo system defeated the world champion in Go\n",
    "\n",
    "In general, RL algorithms have been limited due to significant data and computational requirements.\n",
    "\n",
    "As a result, many well-known use cases involve learning to play games. More recently, progress has been made in areas with more direct business applications.\n",
    "\n",
    "## Reinforcement Learning Architecture\n",
    "\n",
    "The main components of reinforcement learning are: Policy, Agents, Actions, State, and Reward.\n",
    "\n",
    "- Solutions represents a Policy by which Agents choose Actions in response to the State\n",
    "\n",
    "- Agents typically maximize expected rewards over time\n",
    "\n",
    "- In Python, the most common library for RL is Open AI GYM\n",
    "\n",
    "This differs from typical Machine Learning Problems:\n",
    "\n",
    "- Unlike labels, rewards are not known and are often highly uncertain\n",
    "\n",
    "- As actions impact the environment, the state changes, which changes the problem\n",
    "\n",
    "- Agents face a tradeoff between rewards in different periods\n",
    "\n",
    "Examples of everyday applications of Reinforcement Learning include recommendation engines, marketing, and automated bidding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Something went wrong with pygame. This should never happen.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/macos/LearningSpace/202308_machine_learning_ibm_coursera/005deep-learning-reinforcement-learning/w9.ReinforcementLearning.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/macos/LearningSpace/202308_machine_learning_ibm_coursera/005deep-learning-reinforcement-learning/w9.ReinforcementLearning.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macos/LearningSpace/202308_machine_learning_ibm_coursera/005deep-learning-reinforcement-learning/w9.ReinforcementLearning.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Render (show) the environment\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macos/LearningSpace/202308_machine_learning_ibm_coursera/005deep-learning-reinforcement-learning/w9.ReinforcementLearning.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[1;32m    328\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/wrappers/env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_render \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_render \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[0;34m(env, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         \u001b[39massert\u001b[39;00m env\u001b[39m.\u001b[39mrender_mode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m env\u001b[39m.\u001b[39mrender_mode \u001b[39min\u001b[39;00m render_modes, (\n\u001b[1;32m    312\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRender mode: \u001b[39m\u001b[39m{\u001b[39;00menv\u001b[39m.\u001b[39mrender_mode\u001b[39m}\u001b[39;00m\u001b[39m, modes: \u001b[39m\u001b[39m{\u001b[39;00mrender_modes\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m         )\n\u001b[0;32m--> 316\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    318\u001b[0m \u001b[39m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/envs/toy_text/taxi.py:290\u001b[0m, in \u001b[0;36mTaxiEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render_text()\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_render_gui(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_mode)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gym/envs/toy_text/taxi.py:309\u001b[0m, in \u001b[0;36mTaxiEnv._render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mSurface(WINDOW_SIZE)\n\u001b[1;32m    308\u001b[0m \u001b[39massert\u001b[39;00m (\n\u001b[0;32m--> 309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m ), \u001b[39m\"\u001b[39m\u001b[39mSomething went wrong with pygame. This should never happen.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock \u001b[39m=\u001b[39m pygame\u001b[39m.\u001b[39mtime\u001b[39m.\u001b[39mClock()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Something went wrong with pygame. This should never happen."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import Library\n",
    "import gym\n",
    "\n",
    "# Make an Environment\n",
    "#env = gym.make(\"environment_type\")\n",
    "env = gym.make(\"Taxi\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# Render (show) the environment\n",
    "env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

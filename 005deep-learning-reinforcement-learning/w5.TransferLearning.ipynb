{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this module, you will understand what is transfer learning and how it works. You will implement transfer learning in 5 general steps using a variety of popular pre-trained CNN architectures, such as VGG-16 and ResNet-50. You will study the differences among those CNN architectures and see how the invention of each solves the problem of its predecessors. Last, but not least, as we are moving to working with deeper neural networks, you will also be equipped with regularization techniques to prevent overfitting of complex models and networks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Understand the concept of transfer learning\n",
    "\n",
    "Describe modern CNN architectures such as ResNet-50 and VGG-16\n",
    "\n",
    "Implement transfer learning using modern CNN architectures\n",
    "\n",
    "Describe how different regularization techniques help prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transfer Learning\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Early layers in a Neural Network are the hardest (i.e. slowest) to train.\n",
    "- Due to vanishing gradient property.\n",
    "- But these \"primitive\" features should be general across many image classification tasks.\n",
    "\n",
    "Later layers in the network are capturing features that are more particular\n",
    "to the specific image classification problem.\n",
    "- Later layers are easier (quicker) to train since adjusting their weights\n",
    "has a more immediate impact on the final result.\n",
    "\n",
    "Famous, Competition-Winning Models are difficult to train from scratch:\n",
    "- Huge datasets (like ImageNet)\n",
    "- Long number of training iterations\n",
    "- Very heavy computing machinery\n",
    "- Time experimenting to get hyper-parameters just right\n",
    "\n",
    "However, the basic features (edges, shapes) learned in the early layers\n",
    "of the network should generalize.\n",
    "- Results of the training are just weights (numbers) that are easy to store.\n",
    "- Idea: keep the early layers of a pre-trained network,\n",
    "and re-train the later layers for a specific application\n",
    "- This is called Transfer Learning.\n",
    "\n",
    "![](./images/28_TransferLearning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning and Fine Tuning\n",
    "\n",
    "### Transfer Learning Options\n",
    "\n",
    "The additional training of a pre-trained network on a specific new dataset\n",
    "is referred to as: \"Fine-Tuning\".\n",
    "- There are different options on \"how much\" and \"how far back\" to fine-tune.\n",
    "- Should I train just the very last layer?\n",
    "- Go back a few layers?\n",
    "- Re-train the entire network (from the starting point of the existing network)?\n",
    "\n",
    "### Guiding Principles for Fine-Tuning\n",
    "\n",
    "While there are no \"hard and fast\" rules, there are some guiding principles to keep in mind:\n",
    "- The more similar your data and problem are to the source data\n",
    "of the pre-trained network, the less fine-tuning is necessary.\n",
    "- E.g. Using a network trained on ImageNet to distinguish \"dogs\" from \"cats\"\n",
    "should need relatively little fine-tuning.\n",
    "- It already distinguished different breeds of dogs and cats,\n",
    "so likely has all the features you will need.\n",
    "\n",
    "### Guiding Principles for Fine-Tuning\n",
    "The more data you have about your specific problem,\n",
    "the more the network will benefit from longer and deeper fine-tuning.\n",
    "- E.g. If you have only 100 dogs and 100 cats in your training data,\n",
    "you probably want to do very little fine-tuning.\n",
    "- If you have 10,000 dogs and 10,000 cats you may get more value\n",
    "from longer and deeper fine-tuning.\n",
    "\n",
    "If your data is substantially different in nature than the data the source model was trained on,\n",
    "Transfer Learning may be of little value.\n",
    "- E.g. A network that was trained on recognizing typed Latin alphabet characters\n",
    "would not be useful in distinguishing cats from dogs.\n",
    "- But it likely would be useful as a starting point for recognizing Cyrillic Alphabet characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architectures - Learning Goals\n",
    "\n",
    "In this section, we will cover:\n",
    "\n",
    "Architectures used in Convolutional Neural Networks\n",
    "\n",
    "Commonly-used network types:\n",
    "\n",
    "- LeNet\n",
    "- AlexNet\n",
    "- VGG\n",
    "- Inception\n",
    "- ResNet\n",
    "\n",
    "## Convolutional Neural Network Architectures - LeNet\n",
    "\n",
    "### LeNet-5\n",
    "- Created by Yann LeCun in the 1990s\n",
    "- Used on the MNIST data set.\n",
    "- Novel Idea: Use convolutions to efficiently learn features on data set.\n",
    "\n",
    "![](./images/29_Lenet.png)\n",
    "\n",
    "![](./images/30_Lenet.png)\n",
    "\n",
    "![](./images/33_Lenet_PoolingLayers.png)\n",
    "\n",
    "![](./images/34_Lenet_CNNLayer2.png)\n",
    "\n",
    "![](./images/35_Lenet_ConnectedLayers.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architectures - AlexNet\n",
    "\n",
    "Created in 2012 for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC).\n",
    "- Task: predict the correct label from among 1000 classes.\n",
    "- Dataset: around 1.2 million images.\n",
    "\n",
    "AlexNet is considered the \"flash point\" for modern deep learning.\n",
    "\n",
    "It Demolished the competition:\n",
    "- Top 5 error rate of 15.4%\n",
    "- Next best: 26.2%\n",
    "\n",
    "![](./images/36_AlexNet.png)\n",
    "\n",
    "### AlexNet - Details\n",
    "\n",
    "AlexNet developers performed data augmentation for training.\n",
    "- Cropping, horizontal flipping, and other manipulations.\n",
    "\n",
    "Basic Template:\n",
    "- Convolutions with ReLUs.\n",
    "- Sometimes add maxpool after convolutional layer.\n",
    "- Fully connected layers at the end before a softmax classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convolutional Neural Network Architectures - Inception\n",
    "\n",
    "Szegedy et al 2014.\n",
    "- Idea: network would want to use different receptive fields.\n",
    "- Want computational efficiency.\n",
    "- Also want to have sparse activations of groups of neurons.\n",
    "\n",
    "Hebbian principle: \"Fire together, wire together\".\n",
    "- Solution: Turn each layer into branches of convolutions.\n",
    "- Each branch handles smaller portion of workload.\n",
    "- Concatenate different branches at the end.\n",
    "\n",
    "![](./images/37_Inception.png)\n",
    "\n",
    "![](./images/38_Inception.png)\n",
    "\n",
    "![](./images/39_Inception.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Convolutional Neural Network Architectures - ResNet\n",
    "\n",
    "![](./images/40_DeeperNetworkIssue.png)\n",
    "\n",
    "Surprising because deeper networks should overfit more.\n",
    "\n",
    "So what's happening?\n",
    "- Early layers of Deep Networks are very slow to adjust.\n",
    "- Analogous to \"Vanishing Gradient\" issue.\n",
    "- In theory, should be able to just have an \"identity\" transformation\n",
    "that makes the deeper network behave like a shallower one.\n",
    "\n",
    "![](./images/41_ResNet.png)\n",
    "\n",
    "\n",
    "![](./images/42_ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Techniques for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this module, you will understand what is transfer learning and how it works. You will implement transfer learning in 5 general steps using a variety of popular pre-trained CNN architectures, such as VGG-16 and ResNet-50. You will study the differences among those CNN architectures and see how the invention of each solves the problem of its predecessors. Last, but not least, as we are moving to working with deeper neural networks, you will also be equipped with regularization techniques to prevent overfitting of complex models and networks.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "Understand the concept of transfer learning\n",
    "\n",
    "Describe modern CNN architectures such as ResNet-50 and VGG-16\n",
    "\n",
    "Implement transfer learning using modern CNN architectures\n",
    "\n",
    "Describe how different regularization techniques help prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Transfer Learning\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Early layers in a Neural Network are the hardest (i.e. slowest) to train.\n",
    "- Due to vanishing gradient property.\n",
    "- But these \"primitive\" features should be general across many image classification tasks.\n",
    "\n",
    "Later layers in the network are capturing features that are more particular\n",
    "to the specific image classification problem.\n",
    "- Later layers are easier (quicker) to train since adjusting their weights\n",
    "has a more immediate impact on the final result.\n",
    "\n",
    "Famous, Competition-Winning Models are difficult to train from scratch:\n",
    "- Huge datasets (like ImageNet)\n",
    "- Long number of training iterations\n",
    "- Very heavy computing machinery\n",
    "- Time experimenting to get hyper-parameters just right\n",
    "\n",
    "However, the basic features (edges, shapes) learned in the early layers\n",
    "of the network should generalize.\n",
    "- Results of the training are just weights (numbers) that are easy to store.\n",
    "- Idea: keep the early layers of a pre-trained network,\n",
    "and re-train the later layers for a specific application\n",
    "- This is called Transfer Learning.\n",
    "\n",
    "![](./images/28_TransferLearning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning and Fine Tuning\n",
    "\n",
    "### Transfer Learning Options\n",
    "\n",
    "The additional training of a pre-trained network on a specific new dataset\n",
    "is referred to as: \"Fine-Tuning\".\n",
    "- There are different options on \"how much\" and \"how far back\" to fine-tune.\n",
    "- Should I train just the very last layer?\n",
    "- Go back a few layers?\n",
    "- Re-train the entire network (from the starting point of the existing network)?\n",
    "\n",
    "### Guiding Principles for Fine-Tuning\n",
    "\n",
    "While there are no \"hard and fast\" rules, there are some guiding principles to keep in mind:\n",
    "- The more similar your data and problem are to the source data\n",
    "of the pre-trained network, the less fine-tuning is necessary.\n",
    "- E.g. Using a network trained on ImageNet to distinguish \"dogs\" from \"cats\"\n",
    "should need relatively little fine-tuning.\n",
    "- It already distinguished different breeds of dogs and cats,\n",
    "so likely has all the features you will need.\n",
    "\n",
    "### Guiding Principles for Fine-Tuning\n",
    "The more data you have about your specific problem,\n",
    "the more the network will benefit from longer and deeper fine-tuning.\n",
    "- E.g. If you have only 100 dogs and 100 cats in your training data,\n",
    "you probably want to do very little fine-tuning.\n",
    "- If you have 10,000 dogs and 10,000 cats you may get more value\n",
    "from longer and deeper fine-tuning.\n",
    "\n",
    "If your data is substantially different in nature than the data the source model was trained on,\n",
    "Transfer Learning may be of little value.\n",
    "- E.g. A network that was trained on recognizing typed Latin alphabet characters\n",
    "would not be useful in distinguishing cats from dogs.\n",
    "- But it likely would be useful as a starting point for recognizing Cyrillic Alphabet characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architectures - LeNet\n",
    "\n",
    "## Convolutional Neural Network Architectures - AlexNet\n",
    "\n",
    "## Convolutional Neural Network Architectures - Inception\n",
    "\n",
    "## Convolutional Neural Network Architectures - ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Techniques for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation Training and Keras\n",
    "\n",
    "In this module, you will learn about the maths behind the popular Back Propagation algorithm used to optimize neural networks. In the Back Propagation notebook, you will also see and understand the use of activation functions. The main purpose of most activation function is to introduce non-linearity in the network so it would be capable of learning more complex patterns. Last, but not least, you will learn to use functions and APIs from the Keras library to solve tasks that involve neural networks, and these tasks start with loading images.\n",
    "\n",
    "Learning Objectives\n",
    "- Understand the working mechanics of Back Propagation\n",
    "- Implement different methods of loading images using Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Back Propagation, Activation Function\n",
    "\n",
    "## How to Train a Neural Network\n",
    "\n",
    "How to Train a Neural Net?\n",
    "\n",
    "- Put in Training inputs, get the output.\n",
    "- Compare output to correct answers: Look at loss function J.\n",
    "- Adjust and repeat.\n",
    "- Backpropagation tells us how to make a single adjustment using calculus.\n",
    "\n",
    "How Have We Trained Before?\n",
    "\n",
    "Gradient Descent:\n",
    "- 1. Make prediction\n",
    "- 2. Calculate Loss\n",
    "- 3. Calculate gradient of the loss function w.r.t. parameters\n",
    "- 4. Update parameters by taking a step in the opposite direction\n",
    "- 5. Iterate\n",
    "\n",
    "How to Train a Neural Net?\n",
    "\n",
    "How could we change the weights to make our Loss Function lower?\n",
    "1. Think of neural net as a function F: X  Y.\n",
    "2. F is a complex computation involving many weights Wk.\n",
    "3. Given the structure, the weights \"define\" the function F (and therefore define our model).\n",
    "4. Loss Function is J(y, F(x)).\n",
    "\n",
    "![](./images/12_TrainNeuralNetwork.png)\n",
    "\n",
    "How to Train a Neural Net?\n",
    "Get $\\dfrac{\\partial J}{\\partial W_k}$ for every weight in the network.\n",
    "This tells us what direction to adjust each $W_k$ if we want to lower our loss function.\n",
    "Make an adjustment and repeat!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "![](./images/13_BackPropagation.png)\n",
    "\n",
    "![](./images/14_Backpropagation.png)\n",
    "\n",
    "###  Vanishing Gradients\n",
    "\n",
    "The mathematical equation is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial W} = (\\hat{y} - y) \\cdot W^{(3)} \\cdot \\sigma'(z^{(3)}) \\cdot W^{(2)} \\cdot \\sigma'(z^{(2)}) \\cdot X\n",
    "$$\n",
    "\n",
    "Points to remember:\n",
    "\n",
    "- Remember $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z)) \\leq .25$.\n",
    "- As we have more layers, the gradient gets very small at the early layers.\n",
    "- This is known as the \"vanishing gradient\" problem.\n",
    "- For this reason, other activations (such as ReLU) have become more common.\n",
    "\n",
    "## The Sigmoid Activation Function\n",
    "\n",
    "## Other Popular Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Library\n",
    "\n",
    "## Popular Deep Learning Library\n",
    "\n",
    "## A Typical Keras Workflow\n",
    "\n",
    "## Implementing an Example Neural Network in Keras"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
